\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage[labelfont=bf]{caption}
\DeclareCaptionFormat{myformat}{#1#2#3\hrulefill}
%\captionsetup[figure]{format=myformat}
\usepackage{tikz}
\usepackage{subfigure}
\usepackage{geometry}
\usepackage{ifthen}
\usepackage{cite}
\usepackage{chngcntr}

\counterwithin{equation}{section}
%\counterwithin{figure}{section}

%\usepackage{todonotes}

\usepackage{titlesec}
\titleformat
{\part}
[display]
{\bfseries\LARGE}
{}
{-2ex}
{\MakeUppercase{\partname{ }}\thepart:{ }}
[\vspace{.5ex}%
]


\usepackage{cite}

\usetikzlibrary{decorations.pathmorphing}

\geometry{margin=3cm}%, bottom=2cm, top=2cm}

\newcommand{\todo}[2][]{\textcolor{red}{TODO\ifthenelse{\equal{#1}{}}{}{[#1]}: #2}}
\newcommand{\done}[2][]{\textcolor{green!50!black}{DONE\ifthenelse{\equal{#1}{}}{}{[#1]}: #2}}
\newcommand{\remark}[2][]{\textcolor{red!70!yellow}{REMARK\ifthenelse{\equal{#1}{}}{}{[#1]}: #2}}
%\newcommand{\todo}[1]{}
\newcommand{\korrektur}[1]{\textcolor{red}{\textbf{Korrektur:} #1}}
\newcommand{\reals}{\mathbb{R}}

\newcommand{\zoomfactor}{0.5}
\newcommand{\autozoomfactor}{0.15}

\newcommand{\figurepage}[1]{
  %\newgeometry{top=0cm, bottom=0cm}
  \input{#1}
  %\restoregeometry
}

% for me, this looks better. uncomment these lines if you do not like it that small
%\renewcommand{\zoomfactor}{0.3}
%\renewcommand{\autozoomfactor}{0.075}

\title{Discontinous Galerkin Methods for Shallow Water Equations\\
  \vspace{.2cm}
\large{Empirical studies on accuracy of quadrature rules for shallow water equations with the discontinous Galerkin method}
}
\author{Stjepan Bakrac \and Philipp M\"uller}
\date{}

\newcommand{\partialderivative}[1]{\dfrac{\partial}{\partial #1}}
\newcommand{\pd}[2]{\dfrac{\partial #1}{\partial #2}}
\newcommand{\totalderivative}[1]{\dfrac{d}{d #1}}
\newcommand{\naturals}{\mathbb{N}}

\renewcommand{\phi}{\varphi}

\newcommand\intend{\mathrm{d}}

\DeclareMathOperator{\divergence}{div}

\begin{document}

\thispagestyle{empty}
% Titelblatt: Gestaltung von der TUM-Informatik-Homepage genommen.
\makeatletter
\begin{titlepage}
\vspace*{\fill}
\begin{center}
  \includegraphics[scale=0.75]{TUM.pdf}

  \vspace{.5cm}
  \textsc{ \Large Fakultät für Informatik\\\vspace{.2cm}
    \large der Technischen Universität München}

  \vspace{1cm}

  \large Interdisciplinary Project in Mathematics and Computer Science

  \vspace{.4cm}

  \LARGE
  \@title

  \vspace{1cm}
  \vfill{}
  \begin{tabular}{ll}
    Authors:         & Stjepan Bakrac, Philipp Müller \\
    Supervisor:      & Univ.-Prof. Dr. Hans-Joachim Bungartz \\
    Advisors:        & Univ.-Prof. Dr. Michael Bader \\
                     & Dipl.-Inf. Martin Schreiber \\
    Submission date: & \today
  \end{tabular}

  \vspace{1cm}

  \begin{tikzpicture}
    \draw[black](95:1) arc (95:445:1);
    \draw[black](0,-.8) -- (0,1.2);
  \end{tikzpicture}
\end{center}
\vspace*{\fill}
\end{titlepage}
\makeatother

\clearpage

\begin{abstract}
  The question how to simulate waves on a computer is subject to many difficulties. At several points in the process of transferring continuous to discrete formulations, approximations have to be made.
One of those points is the discretization of the continuous space into finite segments.
Yet another point is not as obvious, and occurs when trying to compute an integral. The complexity is of such degree that it is usually not feasible to compute exactly, and for that another approximation-error is introduced.

Despite all of this, the accuracy loss can normally be minimized by sacrificing computing time.
However, the specific approximation of an integrated fractional function using Gaussian quadrature has to be analyzed separately.
It occurs when computing how much of the relevant quantities are passed from one segment to another, which, due discontinuities along the separating lines, has to be handled in a way that preserves as much accuracy as possible.

Based on the shallow water equations, this work aims to explain the derivation of the discretization, the necessary equations for computing those quantities, and gives insight to the quadrature error by comparing the analytical solution to the approximate solution using Gaussian quadrature employed at various points.
\end{abstract}

\hypersetup{
    colorlinks=false,
    pdfborder={0 0 0},
}

\newpage
\setcounter{tocdepth}{2}
\tableofcontents{}

\hypersetup{
    colorlinks=false,
    pdfborder={0 0 1},
}

\newpage
\part{Overview of the simulation approach}
\label{part:introduction}

This project deals with the issue of water simulation, especially tsunami simulation.
The aim is to describe the steps in deriving the equations necessary for water simulation, as well as analyze accuracy losses by certain approximations used as a trade-off for increases in simulation speed.
Eventually, we will present the results and judge the methods employed by our observations.

When trying to simulate wave propagation over a certain domain, several limitations have to be considered.
Over time, various equations that deal with problems of this kind have been derived, each set suited for different situations.
The main problem for computer simulation is that computers cannot represent the full continuous domain perfectly, and so the entire domain has to be discretized.
This discretization is commonly done with a set of cells that contain several fields of information (such as mass/height, velocity, etc.).
Aside from the domain discretization, accurate simulation of the wave propagation itself is also a challenging task, making the simulation even more computationally complex and expensive.
Especially in tsunami-related environments even non-local factors like bathymetry information, tidal forces and the Coriolis effect have to be considered to get accurate results.

In this work we focus on computations based on the shallow water equations with the discontinuous Galerkin method, both of which will be described below.
Along with that, we will provide an analysis of the discretization problem, as well as explore some numerical solutions for it.
We will not discretize the domain ourselves, but instead work with a framework developed at the TUM, Sierpinski.
This framework uses an adaptive triangle grid to discretize the domain.
However, adaptivity will not be relevant to us, as we will only examine communication between at most two neighboring triangles which share the same edge.
As such, our results can be applied to both adaptive and non-adaptive grids.

\section{Shallow water equations}
\label{sec:shallow-water-equations}

A set of equations dealing specifically with wave propagation are the so-called shallow water equations. However, contrary to their name, they can be applied to arbitrary-depth scenarios, as long as the horizontal scale is significantly greater than the vertical scale. This makes them suitable for wave simulation over a greater domain, which is of specific interest for tsunami simulation.

The essence of the equations is based on the conservation law \cite{leveque2002finite} (with regards to mass and momentum conservation) as presented in the general continuity equation.
In its differential form it looks like this:

\begin{equation}
  \label{eq:general-continuity-equation}
  \pd{\mathbf{q}}{t} + \nabla \cdot F(\mathbf{q}) = S
\end{equation}

\begin{itemize}
\item[$\mathbf{q}$] The state vector. It contains all relevant information for any given point in space.
\item[$F$] The flux function. Applied to $\mathbf{q}$ it results in a vector describing the spatial transport of the quantities stored in $\mathbf{q}$.
\item[$S$] A source term. This is used as an offset for the computed quantities. What this quantity means can differ depending on the situation. In the context of water simulation, it represents the bathymetry, which will be set to 0 for this project, meaning we only consider constant bathymetry. In reality this value does not follow a formula, but is derived through empirical measurements for every point in the domain.
\end{itemize}

The shallow water equations can be applied to the general continuity equation by choosing appropriate variables. For the state vector $\mathbf{q}$ we use the following:

\begin{eqnarray*}
  \mathbf{q} =
  \begin{pmatrix}
    h \\ h v_x \\ h v_y
  \end{pmatrix}
\end{eqnarray*}

Here $h$ is the height of the water at that point, which is proportional to the mass of the domain. $v_x$ and $v_y$ are the velocities in the $x$ and $y$ direction. We will use $\mathbf{v}$ to denote the two-dimensional vector consisting of $\left( v_x, v_y \right)^T$. The flux function can be defined as follows:

\begin{eqnarray*}
  F(\mathbf{q}) =
  \begin{pmatrix}
    h \mathbf{v} \\ h v_x \mathbf{v} + \frac{1}{2} g h^2 e_x \\ h v_y \mathbf{v} + \frac{1}{2} g h^2 e_y
  \end{pmatrix}
\end{eqnarray*}

This takes into account gravitational effects for the respective $x$ and $y$ components. This is accomplished by $e_x$ and $e_y$, which are $\left(1, 0\right)^T$ and $\left(0, 1\right)^T$ respectively, thus only adding the gravitational terms where necessary.

As was mentioned before, the source term will be set to zero throughout this project, which represents constant bathymetry, hence $S=\mathbf{0}$.

Having defined those terms to suit our purpose, we can rewrite the above as follows:

\begin{eqnarray}
  \label{eqn:shallow-water-flux}
  \mathbf{q} =
  \begin{pmatrix}
    h \\ u_x \\ u_y
  \end{pmatrix} \quad
  F(\mathbf{q}) =
  \begin{pmatrix}
    \mathbf{u} \\ \frac{u_x}{h}\mathbf{u} + \frac{1}{2} g h^2 e_x \\ \frac{u_y}{h}\mathbf{u} + \frac{1}{2} g h^2 e_y
  \end{pmatrix}
\end{eqnarray}

Here we replaced the velocity component $\mathbf{v}h$ by a momentum component $\mathbf{u}$, which, similarly, consists of two dimensions, one in each direction (referred to by $u_x$ and $u_y$). Occasionally we will need only the $x$ components (or $y$ components) from that vector, for which we will write $F^x$ (and $F^y$ respectively):

\begin{eqnarray*}
  F^x(\mathbf{q}) =
  \begin{pmatrix}
    u_x \\ \frac{u_x^2}{h} + \frac{1}{2} g h^2 \\ \frac{u_x u_y}{h}
  \end{pmatrix}
  \quad
  F^y(\mathbf{q}) =
  \begin{pmatrix}
    u_y \\ \frac{u_x u_y}{h} \\ \frac{u_y^2}{h} + \frac{1}{2} g h^2
  \end{pmatrix}
\end{eqnarray*}

\section{Discontinuous Galerkin}
\label{sec:discontinuous-galerkin}

The numerical method we are using to solve the shallow water equations on our grid is the discontinuous Galerkin method (DG). It combines ideas from the finite element method and the finite volume method. The first suggests computing solutions for each element locally, which is what the DG method also does. However, it lets adjacent elements communicate relevant information with each other by numerically computing a flux function, which determines how much of each stored quantity is passed along between elements, which is a method borrowed from finite volume schemes.

Similar to the finite element method, we multiply the equation (in this case the shallow water equations) by a test function, then integrate over the entire domain. This can be done with more than one function as well, and we will later use several, depending on the degree of the functions we use. The functions we choose for that will be called \emph{basis functions}, as they represent a nodal basis in the corresponding polynomial space. Details of what they look like and how we will derive them are explained in section \ref{sec:basis-functions-choice}. What we obtain is the following (with $\phi_i$ being the $i$th basis function), called the weak form:

\begin{equation}
  \label{eq:general-continuity-equation-discontinuous-galerkin}
  \int_\Omega \pd{\mathbf{q}}{t} \phi_i \,d\Omega + \int_\Omega \nabla \cdot F(\mathbf{q}) \phi_i \,d\Omega = \int_\Omega S \phi \,d\Omega
\end{equation}

With the bathymetry (our source term, $S$) at zero, this leads to:

\begin{equation}
  \label{eq:general-continuity-equation-discontinuous-galerkin-no-bathymetrity}
  \int_\Omega \pd{\mathbf{q}}{t} \phi_i \,d\Omega + \int_\Omega \nabla \cdot F(\mathbf{q}) \phi_i \,d\Omega = 0
\end{equation}

The operator $\nabla \cdot$ is the divergence operator, computing the sum of the $x$ derivative of the first component and the $y$ derivative of the second component. Using the operator $\nabla$ and applying the Gaussian divergence theorem, we can transform the equation \ref{eq:general-continuity-equation-discontinuous-galerkin-no-bathymetrity} above to

\begin{equation}
  \label{eq:shallow-water-weak-form-div-applied}
  \int_\Omega \pd {\mathbf{q}}{t} \phi \, d\Omega +
  \int_{\partial \Omega} F(\mathbf{q}) \cdot \mathbf{n} \, \phi \, ds -
  \int_\Omega F(\mathbf{q}) \cdot \nabla \phi \, d\Omega = 0.
\end{equation}

As we can see, the second integral is now over $\partial \Omega$ and $ds$, which describes the integral over the border of that domain.
Note at this point, that these are actually three equations, one for each component of $\mathbf{q}$, each of which contains two-dimensional vectors for the two space coordinates.
The dot product here signifies a scalar product, i.e.\,multiply the components by row and add up the result.
The last integral contains the symbol $\nabla$, which denotes the gradient of $\phi$.
This operator takes a function and creates a vector from it containing the derivatives for $x$ and $y$ in the respective components.

\section{Discretization}
\label{sec:discretization}

Since we will need to discretize the space eventually, we will have to work on a disjoint set of polygons, together incorporating the entire simulation space.
In our case, this will be a triangle grid, so we will use triangles to simplify the equations further.

We can apply the discontinuous Galerkin method for every triangle individually, meaning we will use a single triangle as the integration domain in our calculations.
Given that, we can use the previous equation (\ref{eq:general-continuity-equation-discontinuous-galerkin-no-bathymetrity}) with regards to our triangle's space (here denoted by $T$ instead of $\Omega$):

\begin{equation}
  \label{eq:shallow-water-weak-form-div-applied-triangle}
  \int_T \pd {\mathbf{q}}{t} \phi \, dT +
  \int_{\partial T} F(\mathbf{q}) \cdot \mathbf{n} \, \phi \, ds -
  \int_T F(\mathbf{q}) \cdot \nabla \phi \, dT = 0.
\end{equation}

The second integral in that equation is an integral over the boundary of one triangle.
The $\mathbf{n}$ in there denotes the outward facing normal at that point.
We write $ds$ to denote integration over the $(x,y)$ values along the border.
Since it is a boundary integral of a triangle, we can split it up into several integrals, one for each edge.
This also implies a normal vector $\mathbf{n}_e$ per edge of the set of edges $E = \{e_1, e_2, e_3\}$.

\begin{equation}
  \label{eq:boundary-integral-sum}
  \int_{\partial P} F(\mathbf{q}) \cdot \mathbf{n} \, \phi \, ds = \sum_{e \in E} \int_{e} F(\mathbf{q}) \cdot \mathbf{n}_e \, \phi \, ds = \sum_{e \in E} \int_{e} F^e \phi \, ds =: \text{Boundary integral}
\end{equation}

We also combined the terms $F(\mathbf{q})$ and $\mathbf{n}$ to one term $F^e$. This term denotes the quantities given or received from the polygon neighboring on the edge $e$. The normal will not pose a problem for us, as we rotate the edges into a uniform reference space, so they can all be computed the same way, regardless of position (described in detail below). As was mentioned in \ref{sec:discontinuous-galerkin}, we apply this method for every basis function at our disposal. Since we have one for each support point, it will run from 1 through $n$, named $\phi_i$ accordingly:

\begin{equation}
  \label{eq:shallow-water-weak-form-div-applied-approximation}
  \int_P \pd {\mathbf{q}}{t} \phi_i \, dP +
  \sum_{e \in E} \int_{e} F^e \phi_i \, ds  -
  \int_P F(\mathbf{q}) \cdot \nabla \phi_i \, dP = 0
  \quad \forall i \in \{1, \dots, n\}
\end{equation}

\section{Reference space}
\label{sec:reference-space}

The actual triangulization method employed will be irrelevant to us, as we will only analyze behavior between two triangles at a time, or even just within one triangle.
The usual approach is either to use a uniform grid for the entire domain, or an adaptive grid, which introduces a few implementation challenges but allows for fine-grain control of simulation parameters, usually improving the accuracy-to-performance ratio.

The advantage of using a uniform space is that with a suitable library for coordinate projection, we can rotate triangles and edges into a space that is relevant for us.
Instead of applying different directional cell update rules for each individual triangle, we apply a transformation to the triangles, which allows us to apply one one set of update rules to all cells.

In addition to rotating, we can also scale the edges of the triangles to always be of a certain length.
Our reference space assumes a leg of length 1 and consequently the hypotenuse of length $\sqrt{2}$.

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}[scale=1.5]
    \draw (-2.5,1.5) -- node[above]{$c$} (-1,1.5) -- node[right]{$c$} (-1,0) -- node[below left]{$\sqrt{2}c$} (-2.5,1.5);
    \draw[->,semithick,decorate,decoration=snake] (-0.3,0.5) -- node[above,yshift=.5em]{Projection} (0.3,0.5);
    \begin{scope}[xshift=2.5em]
      \draw (0,0) -- node[left]{1} (0,1) -- node[right]{$\sqrt{2}$} (1,0) -- node[below]{1} (0,0);
    \end{scope}

  \end{tikzpicture}
  \caption{A sample triangle rotated into normal space and scaled down to leg length 1.}
  \label{fig:triangle-projection}
\end{figure}

The domain of the entire triangle is given by $T$ (the set of all points on the triangle), and the set of edges $E$, enumerated with $e_i$, $\forall i \in \{1,2,3\}$.
Points on the triangle (both on the edge and inside) will be denoted by $p$.

\begin{figure}[ht]
  \centering
  \subfigure[Degree 0]{
    \begin{tikzpicture}[scale=2]
      \draw[opacity=0.5] (0,0) -- node[left]{$e_3$} (0,1) -- node[right]{$e_2$} (1,0) -- node[below]{$e_1$} (0,0);
      \draw[fill=black] (0.33333333, 0.3333333333) circle (0.01);
    \end{tikzpicture}
  }
  \subfigure[Degree 1]{
    \begin{tikzpicture}[scale=2]
      \draw[opacity=0.5] (0,0) -- node[left]{$e_3$} (0,1) -- node[right]{$e_2$} (1,0) -- node[below]{$e_1$} (0,0);
      \draw[fill=black] (0.5, 0) circle (0.01);
      \draw[fill=black] (0, 0.5) circle (0.01);
      \draw[fill=black] (0.5, 0.5) circle (0.01);
    \end{tikzpicture}
  }
  \subfigure[Degree 2]{
    \begin{tikzpicture}[scale=2]
      \draw[opacity=0.5] (0,0) -- node[left]{$e_3$} (0,1) -- node[right]{$e_2$} (1,0) -- node[below]{$e_1$} (0,0);
      \draw[fill=black] (0.5, 0) circle (0.01);
      \draw[fill=black] (0, 0.5) circle (0.01);
      \draw[fill=black] (0.5, 0.5) circle (0.01);
      \draw[fill=black] (0, 0) circle (0.01);
      \draw[fill=black] (1, 0) circle (0.01);
      \draw[fill=black] (0, 1) circle (0.01);
    \end{tikzpicture}
  }
  \subfigure[Degree 3]{
    \begin{tikzpicture}[scale=2]
      \draw[opacity=0.5] (0,0) -- node[left]{$e_3$} (0,1) -- node[right]{$e_2$} (1,0) -- node[below]{$e_1$} (0,0);
      \draw[fill=black] (0.501426509658179, 0.24928674517091) circle (0.01);
      \draw[fill=black] (0.24928674517091, 0.24928674517091) circle (0.01);
      \draw[fill=black] (0.24928674517091, 0.501426509658179) circle (0.01);
      \draw[fill=black] (0.873821971016996, 0.063089014491502) circle (0.01);
      \draw[fill=black] (0.063089014491502, 0.063089014491502) circle (0.01);
      \draw[fill=black] (0.063089014491502, 0.873821971016996) circle (0.01);
      \draw[fill=black] (0.053145049844817, 0.310352451033784) circle (0.01);
      \draw[fill=black] (0.310352451033784, 0.636502499121399) circle (0.01);
      \draw[fill=black] (0.636502499121399, 0.053145049844817) circle (0.01);
      \draw[fill=black] (0.310352451033784, 0.053145049844817) circle (0.01);
    \end{tikzpicture}
  }
  \caption{Triangles containing support points for polynomial degrees 0 through 3.}
  \label{fig:triangle-support-points-var-degrees}
\end{figure}

We will choose certain points to represent the domain of the triangle called \emph{support points}.
The number of support points depends on the degree of the test functions, as will be discussed in section \ref{sec:basis-functions-choice}.
We will require several such functions for the discontinuous Galerkin method, which is the numerical method we use in this project.
For each test function used for the method we will need one point in the triangle to save the computed information to.
They can be regarded as sample points that are representative for the values within the triangle.

In our case, the test functions will be polynomials of a certain degree.
The higher polynomial degree we operate on, the more support points we will have.
While this increases accuracy, it also makes the involved equations more complex and complicates (or even disallows) certain simplifications, so a proper balance is desired.

Another set of points are Gaussian quadrature points, which we will later use to approximate integrals along the edges.
Accordingly, they will also be located on the respective edges.
They will be explained in more detail in section \ref{sec:boundary-integral}.

\section{Dealing with integrals}
\label{sec:matrix-extraction}

Several pitfalls make the computation of the derived terms in equation \ref{eq:shallow-water-weak-form-div-applied-approximation} difficult.
In general, integrals are rarely easy to calculate, even less so when dealing with rational functions, as they appear in our equation.

To actually compute it, we need to approximate the status vector $\mathbf{q}$, as it's impossible to derive an accurate representation for this value that changes continuously over the domain.
We do that by applying the Gaussian quadrature over the previously mentioned support points.
For that we will use the support points as integration points.
We obtain the vector of the approximated status values over the integration points, weighted by the basis functions:

\begin{equation}
  \label{eq:support-point-approximation}
  \mathbf{q} \approx \sum_{i=1}^n \mathbf{q}_i \phi_i\left(x,y\right)
\end{equation}

The values $\mathbf{q}_i$ are the $\mathbf{q}$ values at the corresponding support points. This helps our computation significantly, since those values do not depend on the position within the domain. That allows us to move them out of the integrals, leaving integrals over various combinations of basis functions. Since these don't depend on $\mathbf{q}$, they can be precomputed and applied to all cells in the domain without a need for individual computation.

Recall that $\mathbf{q}$ (and all $\mathbf{q}_i$) contain three components, the height $h$, the momentum in the $x$ direction $u_x$ and the $y$ momentum $u_y$. This means, equation (\ref{eq:support-point-approximation}) yields three equations (one for each of the components).

Having approximated $\mathbf{q}$ now, there are various matrices we can extract that help us compute the integrals.

\subsection{Mass matrix}
\label{sec:mass-matrix}

The first integral of (\ref{eq:shallow-water-weak-form-div-applied-approximation}) shows exactly how this works. As only the basis functions $\phi_i$ are dependent on $(x,y)$, we can extract $\mathbf{q}$ and obtain a constant term:

\begin{eqnarray*}
  \int_T \pd {\mathbf{q}}{t} \phi_i \, dT & \approx &
  \int_T \pd {\left( \sum_{j=1}^n \mathbf{q}_j \phi_j \right) }{t} \phi_i \, dT = \\
  & = & \sum_{j=1}^n \underbrace{\int_T \phi_i \phi_j \, dT}_{m_{ij}} \pd{\mathbf{q}_j}{t}
\end{eqnarray*}

The elements $m_{ij}$ form the $n \times n$ mass matrix $M$. That means we have a product between $M$ and a vector containing all $\mathbf{q}$ components differentiated by $t$. For a shorthand notation, we can define $\tilde{\mathbf{q}} := \left( \mathbf{q}_1 , \dots , \mathbf{q}_n \right)^T$. Using this, we can now translate the above into the following formula:

\begin{equation*}
  \int_T \pd {\mathbf{q}}{t} \phi_i \, dT \approx
  M \cdot \pd{
    \tilde{\mathbf{q}}}{t}
\end{equation*}

\subsection{Stiffness matrix}
\label{sec:stiffness-matrix}

We will skip the second integral for now and move to the third. We need to treat each of the three components differently. In this section we will see the state vector array splitting up by their coordinates and giving us different results for the $x$ and $y$ components. This means we will be using the $F^x\left(\mathbf{q}\right)$ and $F^y\left(\mathbf{q}\right)$ terms defined in \ref{sec:shallow-water-equations}.

Recall some of the notation we introduced earlier:
\begin{itemize}
\item $\phi_j$ is the $j$th basis function ($j$ ranging from 1 to $n$)
\item $u_{x,j}$ denotes the weight for basis function $\phi_j$ in the approximation of $u_x$
\item $u_{y,j}$ analogous to $u_{x,j}$
\item $h_j$ denotes the weight for basis function $\phi_j$ in the approximation of $h$
\end{itemize}

\subsubsection{First line}
\label{sec:stiffness-matrix-first-line}

Using the same method as in \ref{sec:mass-matrix}, we obtain:

\begin{eqnarray*}
  \int_T F_1(\mathbf{q}) \cdot \nabla \phi \, dT & = &
  \int_T
  % Allgemeine Formulierung schaut viel komplizierter und beschissener aus!
  % \begin{pmatrix}
  %   F_{1,1}(\mathbf{q}) \\ F_{1,2}(\mathbf{1})
  % \end{pmatrix}
  % \cdot \nabla \phi_i \, dT \approx \\
  % &=& \int_T
  % \begin{pmatrix}
  %   \sum_{j=1}^n F_{1,1}(\mathbf{q}_j)\cdot \phi_j \\
  %   \sum_{j=1}^n F_{1,2}(\mathbf{q}_j)\cdot \phi_j \\
  % \end{pmatrix}
  % \cdot
  % \begin{pmatrix}
  %   \pd{\phi_i}{x} \\
  %   \pd{\phi_i}{y}
  % \end{pmatrix} dT \\
  % & = & \sum_{j=1}^n F_{1,1}(\mathbf{q}_j) \underbrace{\int_T \phi_j \pd{\phi_i}{x} \, dT}_{s_{ij}^x} + \sum_{j=1}^n F_{1,2}(\mathbf{q}_j) \underbrace{\int_T \phi_j \pd{\phi_i}{y} \, dT}_{s_{ij}^y}
  \begin{pmatrix}
    u_x \\ u_y
  \end{pmatrix}
  \cdot \nabla \phi_i \, dT \approx \\
  &=& \int_T
  \begin{pmatrix}
    \sum_{j=1}^n u_{x,j} \phi_j \\
    \sum_{j=1}^n u_{y,j} \phi_j \\
  \end{pmatrix}
  \cdot
  \begin{pmatrix}
    \pd{\phi_i}{x} \\
    \pd{\phi_i}{y}
  \end{pmatrix} dT \\
  & = & \sum_{j=1}^n u_{x,j} \underbrace{\int_T \phi_j \pd{\phi_i}{x} \, dT}_{s_{ij}^x} + \sum_{j=1}^n u_{y,j} \underbrace{\int_T \phi_j \pd{\phi_i}{y} \, dT}_{s_{ij}^y}
\end{eqnarray*}

Again we obtain matrices, denoted by the elements $s_{ij}^x$ and $s_{ij}^y$, stored in $S^x$ and $S^y$ respectively (called the stiffness matrices \cite{schwaiger08adaptive}). As before, this computation can also be regarded as a matrix multiplication.

\subsubsection{Second line}
\label{sec:stiffness-second-line}

An analogous approach to the second and third line gives us the following (we do not actually substitute the values for $F_2^x(\mathbf{q})$ and $F_2^y(\mathbf{q})$ because that would just unnecessarily complicate the equations):

\begin{eqnarray}
  \label{eq:third-integral-second-line-1}
  \int_T F_2(\mathbf{q}) \cdot \nabla \phi \, dT & = &
  \int_T
  \begin{pmatrix}
    F_2^x(\mathbf{q}) \\ F_2^y(\mathbf{q})
  \end{pmatrix}
  \cdot \nabla \phi_i \, dT \\
  \label{eq:third-integral-second-line-2}
  & \approx &
  \int_T
  \begin{pmatrix}
    \sum_{j=1}^n F_2^x(\mathbf{q}_j) \phi_j \\
    \sum_{j=1}^n F_2^y(\mathbf{q}_j) \phi_j \\
  \end{pmatrix}
  \cdot
  \begin{pmatrix}
    \pd{\phi_i}{x} \\
    \pd{\phi_i}{y}
  \end{pmatrix} dT \\
  & = & \nonumber \sum_{j=1}^n F_2^x(\mathbf{q}_j) \cdot \int_T \phi_j \pd{\phi_i}{x} \, dT
   {}  + \nonumber \sum_{j=1}^n F_2^y(\mathbf{q}_j) \cdot \int_T \phi_j \pd{\phi_i}{y} \, dT
  % hier das Zeug, aber wenn alle Flussterme eingesetzt sind!
  % \int_T
  % \begin{pmatrix}
  %   \frac{u_x^2}{h} + \frac{1}{2} g h^2 \\ \frac{u_x u_y}{h}
  % \end{pmatrix}
  % \cdot \nabla \phi_i \, dT \\
  % \label{eq:third-integral-second-line-2}
  % & \approx &
  % \int_T
  % \begin{pmatrix}
  %   \sum_{j=1}^n \left(\frac{u_{x,j}^2}{h_j^2} + \frac{1}{2} g h_j^2\right) \phi_j \\
  %   \sum_{j=1}^n \left(\frac{u_{x,j} u_{y,j}}{h_j}\right) \phi_j \\
  % \end{pmatrix}
  % \cdot
  % \begin{pmatrix}
  %   \pd{\phi_i}{x} \\
  %   \pd{\phi_i}{y}
  % \end{pmatrix} dT \\
  % & = & \nonumber \sum_{j=1}^n \left(\frac{u_{x,j}^2}{h_j^2} + \frac{1}{2} g h_j^2\right) \int_T \phi_j \pd{\phi_i}{x} \, dT \\
  % & {} & + \nonumber \sum_{j=1}^n \left(\frac{u_{x,j} u_{y,j}}{h_j}\right) \int_T \phi_j \pd{\phi_i}{y} \, dT
\end{eqnarray}

The resulting terms here, $\int_T \phi_j \pd{\phi_i}{x} \, dT$ and $\int_T \phi_j \pd{\phi_i}{y} \, dT$, are the same stiffness matrices we computed before, $S^x$ and $S^y$ respectively.

\paragraph{Explanation: Sum approximation}
\label{par:sum-approx}

At this point, we need to note an error that we introduce in the approximation from (\ref{eq:third-integral-second-line-1}) to (\ref{eq:third-integral-second-line-2}).

To compute a function $f(h, u_x, u_y)$, instead of computing
\begin{equation*}
  f\left(\sum_{i=1}^n h_i \phi_i,
    \sum_{i=1}^n u_{x,i} \phi_i,
    \sum_{i=1}^n u_{y,i} \phi_i\right),
\end{equation*}
we compute the value
\begin{equation*}
  \sum_{i=1}^n f(h_i,u_{x,i},u_{y,i}) \phi_i.
\end{equation*}

The accuracy of this varies depending on $f$. However, as shown in \cite{cockburn1999discontinuous}, this approximation can be used in practice and give results well within the accepted error margin. It will also be the main part of analysis of part \ref{part:stiffness-matrix}.

\subsubsection{Stiffness matrix, third line}

The third line is computationally equivalent to the previous, with the respective values switched. The terms $S^x$ and $S^y$ appear again:

\begin{eqnarray*}
  \int_T F_3\left(\mathbf{q}\right) \cdot \nabla \phi \, dT & = &
  \int_T
  \begin{pmatrix}
    F_3^x(\mathbf{q}) \\ F_3^y(\mathbf{q})
  \end{pmatrix}
  \cdot \nabla \phi_i \, dT \\
  & \approx & \int_T
  \begin{pmatrix}
    \sum_{j=1}^n F_3^x(\mathbf{q}_j)\cdot \phi_j \\
    \sum_{j=1}^n F_3^y(\mathbf{q}_j)\cdot \phi_j \\
  \end{pmatrix}
  \cdot
  \begin{pmatrix}
    \pd{\phi_i}{x} \\
    \pd{\phi_i}{y}
  \end{pmatrix} dT \\
  & = & \sum_{j=1}^n F_3^x(\mathbf{q}_j) \int_T \phi_j \pd{\phi_i}{x} \, dT
  {} + \sum_{j=1}^n F_3^y(\mathbf{q}_j) \int_T \phi_j \pd{\phi_i}{y} \, dT
  % \int_T
  % \begin{pmatrix}
  %   \frac{u_x u_y}{h} \\ \frac{u_y^2}{h} + \frac{1}{2} g h^2
  % \end{pmatrix}
  % \cdot \nabla \phi_i \, dT \approx \\
  % & = & \int_T
  % \begin{pmatrix}
  %   \sum_{j=1}^n \left(\frac{u_{x,j} u_{y,j}}{h_j}\right) \phi_j \\
  %   \sum_{j=1}^n \left(\frac{u_{y,j}^2}{h_j^2} + \frac{1}{2} g h_j^2\right) \phi_j \\
  % \end{pmatrix}
  % \cdot
  % \begin{pmatrix}
  %   \pd{\phi_i}{x} \\
  %   \pd{\phi_i}{y}
  % \end{pmatrix} dT \\
  % & = & \sum_{j=1}^n \left(\frac{u_{x,j} u_{y,j}}{h_j}\right) \int_T \phi_j \pd{\phi_i}{x} \, dT \\
  % & {} & + \sum_{j=1}^n \left(\frac{u_{y,j}^2}{h_j^2} + \frac{1}{2} g h_j^2\right) \int_T \phi_j \pd{\phi_i}{y} \, dT
\end{eqnarray*}

We again observe the stiffness matrices $S^x$ resp. $S^y$.

\subsection{Boundary integral: Gaussian quadrature}
\label{sec:boundary-integral}

Evaluating $\sum_{e \in E} \int_{e} F^e \phi_i \, ds$ poses a challenge, because the term $F^e$ is located and evaluated on the edge, and hence cannot be moved out of the integral, so the same techniques employed with the previous two integrals will not work here. Instead, we employ a Gaussian quadrature and integrate numerically. To do that we substitute the integral as follows:
\begin{equation}
  \int_{e} F^e \phi_i \, ds \approx \sum_{k=1}^{m} F^e\left(p_k^e\right) \underbrace{\phi_i\left(p_k^e\right) \cdot w_k}_{w_{ik}^e},
\end{equation}
where $m$ is the number of integration points and $w_k$ denotes the Gauss weight for point $p_k^e$ for an arbitrary edge $e$ as the weight is determined normalized in one dimension, and as such is the same for all edges. This also contains the edge length (which was normalized to 1, but the hypotenuse still has length $\sqrt{2}$ in that case), which is why it has to be stored separately for every edge. Similar to the previous examples, the values $\phi_i\left(p_k^e\right) \cdot w_k$ can be precomputed for every edge as well, and added onto the term $F^e\left(p_k^e\right)$ when required. $F^e\left(p_k^e\right)$ itself is called the \emph{numerical flux} and can be computed in different ways, which we will get to in part \ref{part:polynomial-comparison}.

The abscissal values and weights of the integration points for Gaussian quadrature are summed up in table \ref{tab:x-coordinates-gauss-quadrature} and can be found in any book on the topic (i.e.\,\cite{hans2009schwarz}). They can be derived from the zeroes of the Legendre polynomials (see \cite{abramowitzstegun1964handbook}). We show these values for the one dimensional case, but they can easily been projected onto the two dimensional case using linear interpolation.

\begin{table}[ht]
  \renewcommand\arraystretch{1.5}
  \centering
  \begin{tabular}[ht]{cll}
    Order & $x$-coordinates & Respective weights \\
    \hline
    2 & $\frac{1}{2}-\frac{1}{6}\cdot \sqrt{3},\  \frac{1}{2}+\frac{1}{6}\cdot \sqrt{3}$ & 1, 1\\
    3 & $-\frac{1}{10}\cdot \sqrt{15}+\frac{1}{2} ,\  0.5,\  \frac{1}{10}\cdot \sqrt{15}+\frac{1}{2}$ & $\frac{5}{9}, \frac{8}{9}, \frac{5}{9}$ \\
    \hline
  \end{tabular}
  \caption{$x$-coordinates of integration points for Gauss-Quadrature if we integrate within the range $\left[ 0,1 \right]$. Coordinates can be scaled to an arbitrary interval and be transformed onto any line using linear interpolation.}
  \label{tab:x-coordinates-gauss-quadrature}
\end{table}

\section{Matrix overview}
\label{sec:matrix-listing}

The matrices we obtained from section \ref{sec:matrix-extraction} are the following:

\begin{description}
\item[Mass matrix]
  \begin{equation}
    \label{eq:mass-matrix}
    M = [m_{ij}]_{n \times n} = \int_T \phi_i \phi_j \ dT
  \end{equation}
\item[Stiffness matrices]
  \begin{equation}
    \label{eq:stiffness-matrix}
    S^x = [s_{ij}^x]_{n \times n} = \int_T \phi_j \pd{\phi_i}{x}\, dT\quad
    S^y = [s_{ij}^y]_{n \times n} = \int_T \phi_j \pd{\phi_i}{y}\, dT
  \end{equation}
\item[Flux matrices]
  \begin{equation}
    \label{eq:edge-matrix}
    W^e = [w_{ik}^e]_{n \times m} = \phi_i(p_k^e) \cdot w_k
  \end{equation}
\end{description}

Using the approximation $\mathbf{q} \approx \sum_{j=1}^n \mathbf{q}_i \phi_i\left(x,y\right)$, along with the previously defined $\tilde{\mathbf{q}}$ (see section \ref{sec:mass-matrix}), we can transform our equation \ref{eq:shallow-water-weak-form-div-applied-approximation} using the matrices defined above to look like this:

\begin{equation}
  \label{eq:swe-matrix-form}
  M \cdot \pd{\tilde{\mathbf{q}}}{t} +
  \sum_{e \in E} \tilde{F^e} W^e -
  \left(F^x(\tilde{\mathbf{q}}) \cdot S^x +
    F^y(\tilde{\mathbf{q}}) \cdot S^y\right) = 0
\end{equation}

Here $\tilde{F^e}$ means a vector of all computed $F_k^e$, similarly to $\tilde{\mathbf{q}}$. This now has the advantage of being easier to handle, both mathematically as well computationally.

\section{Time stepping}
\label{sec:computing-integrals}

For time stepping, we use the classical Euler method. While there are other techniques such as Runge-Kutta, we still use it to demonstrate the technique of time stepping, since this problem does not affect our analysis (see \cite{hans2009schwarz} for more information).

\subsection{Explicit Euler}
\label{subsec:explicit-euler}

Given an equation of the form
\begin{eqnarray}
  \label{eq:euler-method-setting}
  \pd{x(t)}{t} & = & f(t, x(t)), \\
  x(t_0) & = & x_0
\end{eqnarray}
we want to have a compute $x(t)$ (as shown in \cite{schwaiger08adaptive}).

This is evaluated iteratively by considering a certain time step size, $\tau$. The smaller the time step size, the more accurately the resulting values will approximate the exact solution. However, the time step is left variable because it can be adjusted to be lower when confronted with highly variable state vectors (for example large observed velocities on a small grid), whereas it can be left higher during phases with little variation or similar velocities to save computation time. The new time is computed simply by adding the variable time step on the current time: $t_{k+1} = t_k + \tau$.

The state vector for the next time step is based on the same vector in the current time step. It can be regarded as taking a small time step in the direction of the development of the state vector, where the direction is given by $\pd{x(t)}{t}$. We obtain the final term for the new state variable $x$:

\begin{equation}
  \label{eq:euler-step-solution}
  x_{k+1} = x_k + \tau f(t_k, x_k), \quad k=0,1,2,\dots
\end{equation}

\subsection{Applying the Euler method to our integrals}
\label{subsec:euler-method-applied}

We can apply this method to our modified equation (\ref{eq:swe-matrix-form}) once we solve for our desired quantity $\tilde{\mathbf{q}}$:

\begin{equation*}
  \pd{\tilde{\mathbf{q}}}{t} =
  M^{-1} \cdot \left(
    F^x(\tilde{\mathbf{q}}) \cdot S^x +
    F^y(\tilde{\mathbf{q}}) \cdot S^y -
    \sum_{e \in E} \tilde{F^e} W^e\right
  )
\end{equation*}

This can only be done if $M$ is actually invertible, which is something we can not be sure of in general, but holds true for the polynomials we choose.
The matrix $M$ depends on the chosen basis functions $\phi_1,\dots,\phi_n$.

This now has the form we need to apply the Euler step, resulting in this:

\begin{equation*}
  \label{eq:swe-euler-step-solution}
  \tilde{\mathbf{q}}_{k+1} =
  \tilde{\mathbf{q}}_{k} +
  \tau \cdot M^{-1} \cdot \left(
    F^x(\tilde{\mathbf{q}}) \cdot S^x +
    F^y(\tilde{\mathbf{q}}) \cdot S^y -
    \sum_{e \in E} \tilde{F^e} W^e\right
  )
\end{equation*}

We have to keep in mind that equation (\ref{eq:swe-euler-step-solution}) is actually a collection for the equations for $\mathbf{q}_1$ through $\mathbf{q}_n$, and each $\mathbf{q}_i$ contains three components ($h$, $u_x$, $u_y$), which means we obtain a set of $3n$ equations in total. The equation for $\mathbf{q}_i$ is then used for updating support point $i$.

\section{Quadrature data for the discontinuous Galerkin method}
\label{sec:basis-functions-choice}

So far we have explained what the discontinuous Galerkin method is and how it is applied to derive the equations needed to compute water simulation for one time step with the given information. We will now see that the accuracy of the model depends on the choice of integration points and basis functions.

\subsection{Construction of basis functions}
\label{sec:basis-functions-choice-polynomial}

For our project, we restricted ourselves to polynomial basis functions (depending on two variables $x$ and $y$) of variable degree $d$. The variables $x$ and $y$ represent coordinates within our triangle.

Those functions are supposed to form a basis for a polynomial space, so that we can construct any polynomial of degree $d$ as a linear combination of these basis functions. We use nodal basis functions, i.e.\,the coefficients of the linear combination coincide with the value at one support point \cite{boeck08discontinuous-galerkin-verfahren}.

A relatively simple way of constructing basis functions is to use Lagrange interpolation
\footnote{For another method involving Jacobi polynomials, see \cite{castro07high-order-ader-fv-dg-numerical-methods, hesthaven2007nodal}}:

\begin{itemize}
\item Choose a certain set of $n$ points $p_1=\left( x_1,y_1 \right)^T$ through $p_n=\left( x_n,y_n \right)^T$ (see \ref{sec:choice-support-points}).
\item Construct function $\phi_i$ such that $\phi_i(p_j) = \delta_{ij} \  \forall i,j \in \{1 \dots n\}$ using Lagrange polynomials (with $\delta_{ij}$ being the Kronecker delta function).
\end{itemize}

The (minimum) number of basis functions $n$ is not arbitrary, but it depends on the degree of the polynomials. If we only employ polynomials of degree 0 (i.e.\,constants), we only need one basis function. Increasing the degree by 1 involves introducing variables $x$ and $y$. Thus, we need three polynomials to construct every polynomial of degree 1, one to create polynomials containing $x$, one to create polynomials containing $y$ and one to create other constant polynomials.

Generalizing this scheme we observe that if we want to use polynomials of degree $d$, a polynomial can contain terms $x^a y^b$ with $0 \leq a+b = k \leq d$.
Since $k$ can range from 0 to $d$, and for every k, there's $k+1$ possibilities for $a$ and $b$, we can derive all possible combinations with a simple sum: $\sum_{k = 0}^d k+1 = \frac{1}{2} \cdot (d+2) \cdot (d+1)$ combinations in total.
This means that we need $\frac{1}{2} \cdot (d+2) \cdot (d+1)$ basis functions to span the function space containing all the polynomials up to degree $d$.

In section \ref{sec:choice-support-points}, we will see what the basis functions will look like if we choose a certain set of support points.

\subsection{Choice of support points}
\label{sec:choice-support-points}

As mentioned before, we construct the basis functions with the help of the same number of support points. However, arbitrarily choosing those can lead to some disadvantages for the basis functions as described above. As the basis functions are derived from the support points by a matrix inversion, choosing those should not be done at random. If the points are in a certain pattern, it will result in a singular matrix and be impossible to invert. And while the probability for that is low, the matrix can still be ill-conditioned if the points are chosen arbitrarily. There are several methods to pick the support points to avoid singular or otherwise ill-conditioned matrices.

and we will choose the Gaussian quadrature points, as we will also use the same quadrature to integrate over the domain.

For low degree polynomials, we choose specific distribution of the support points within the triangle:
\begin{description}
\item[Degree 0] We choose the support point at the center of the triangle at $\left(\frac{1}{3}, \frac{1}{3}\right)^T$.
\item[Degree 1] We choose the support points at the center of each edge.
\item[Degree 2] We choose the support points at the triangle corners and at the center of each edge.
\end{description}

For higher degree, we choose the quadrature points derived by Dunavant (see \cite{dunavant1985high}). The distributions of support points are illustrated in figure \ref{fig:triangle-support-points-var-degrees}.

Table \ref{tab:basis-functions-for-low-degrees} shows the basis functions for orders 0 to 2.

\begin{table}[ht]
  \centering
  \begin{tabular}{cclll}
    \toprule
    Order & $n$ & Basis functions & $x$-derivative & $y$-derivative \\
    \midrule
    \multirow{1}{*}{0} & 1 & 1 & 0 & 0\\
    \midrule
    \multirow{3}{*}{1} & \multirow{3}{*}{3} & $1-2y$ & 0 & -2\\
    &  & $-1+2x+2y$ & 2 & 2\\
    &  & $1-2x$ & -2 & 0\\
    \midrule
    \multirow{6}{*}{2} & \multirow{6}{*}{6} & $1-3\,x-3\,y+2\,{x}^{2}+4\,xy+2\,{y}^{2}$ & $-3+4x+4y$ & $-3+4x+4y$\\
& & $4\,x-4\,{x}^{2}-4\,xy$ & $4-8x-4y$ & $-4x$\\
& & $-x+2\,{x}^{2}$ & $-1+4x$ & 0\\
& & $4\,xy$ & $4y$ & $4x$\\
& & $-y+2\,{y}^{2}$ & $0$ & $-1+4y$\\
& & $4\,y-4\,xy-4\,{y}^{2}$ & $-4y$ & $4-4x-8y$ \\
    \bottomrule
  \end{tabular}
  \caption{Basis functions for low degree. While other basis functions can be derived by different means, we will focus on these throughout this project. The second column ($n$) denotes the number of basis functions used for the respective order. See \cite{castro07high-order-ader-fv-dg-numerical-methods,dunavant1985high} for more information.}
  \label{tab:basis-functions-for-low-degrees}
\end{table}

\subsubsection{Symmetry in basis functions}
\label{sec:basis-function-symmetry}

As can be seen in table \ref{tab:basis-functions-for-low-degrees}, there are some symmetries for the basis functions. We will point out some of these symmetries here because they will help us in the analysis later.

\paragraph{Order 1}

Here we see that the third and the first basis function look somewhat similar and are reversed w.r.t,\,their arguments: $\phi_1(x,y)=\phi_3(y,x)$. This same symmetry also holds for their respective derivatives. In that sense, the second basis function can be considered symmetric to itself.

\paragraph{Order 2}

Analogously to the first order, we notice argument-reversed symmetries involving all basis functions:

\begin{eqnarray*}
  \phi_2(x,y) & = & \phi_6(y,x) \\
  \phi_3(x,y) & = & \phi_5(y,x) \\
  \phi_1(x,y) & = & \phi_1(y,x) \\
  \phi_4(x,y) & = & \phi_4(y,x)
\end{eqnarray*}

The same pattern does not hold for higher orders, as it relied on points being chosen in a certain pattern, which differs for order 3 and higher. However, these symmetries will merely help with displaying certain plots, and will not actually be relevant for the error estimation.

\subsubsection{Other properties}
\label{sec:basis-functions-other-properties}

We can derive the following two identities:

\begin{equation}
  \label{eq:sum-basis-functions-is-1}
  \sum_{i=1}^n \phi_i(x,y) = 1
\end{equation}

From this, we can easily follow:

\begin{equation}
  \label{eq:sum-basis-functions-squared}
  \left( \sum_{i=1}^n a \cdot \phi_i(x,y) \right)^2 = a^2
\end{equation}

The first equation can be derived by first considering that we have basis functions $\phi_1$ through $\phi_n$, of order $o$ (hence $n=\frac{(o+1)(o+2)}{2}$). If we then take the sum $\sum_{i=1}^n \phi_i(x,y)$ we obtain a polynomial of degree $o$. This polynomial has exactly $n$ coefficients that may be obtained by solving a system of linear equations containing $n$ equations. These equations are obtained using the $n$ support points, i.e.\,we have a system of $n$ equations and $n$ coefficients, which means that all the coefficients are determined uniquely (if a solution exists).

Note that for any support point $(x_p,y_p)$, we have that

\begin{equation}
  \sum_{i=1}^n \phi_i(x_p,y_p) = 1.
\end{equation}

From this -- and using the fundamental theorem of algebra -- we can conclude that the polynomial $\sum_{i=1}^n \phi_i(x,y)$ must be 1.

% Moreover, we know that the polynomial $\overline{\rho}(x,y)=1$ is a polynomial that fulfils $\overline{\rho}(x_p,y_p)=1$ for all support points $(x_p,y_p)$. Since the coefficients for the polynomial are uniquely determined we know that $\sum_{i=1}^n \phi_i(x,y)$ must be the same as $\rho$.

% It must be the same because of the following: The sum of all basis functions $\phi_i$ yields a new polynomial, whose coefficients must be all zero, except the ``constant coefficient'' that must be 1. This can be seen as follows: Imagine we would like to solve the following system of linear equations:

% \setcounter{MaxMatrixCols}{20}

% \begin{equation*}
%   \begin{pmatrix}
%     1 & x_1 & y_1 & x_1^2 & x_1 y_1 & y_1^2 & x_1^3 & x_1^2 y_1 & x_1 y_1^2 & y_1^3 & \dots & y_1^n \\
%     1 & x_2 & y_2 & x_2^2 & x_2 y_2 & y_2^2 & x_2^3 & x_2^2 y_2 & x_2 y_2^2 & y_2^3 & \dots & y_2^n \\
%     \vdots & &&&&&&&&&& \vdots \\
%     1 & x_n & y_n & x_n^2 & x_n y_n & y_n^2 & x_n^3 & x_n^2 y_n & x_n y_n^2 & y_n^3 & \dots & y_n^n \\
%   \end{pmatrix} \cdot
%   \begin{pmatrix}
%     \alpha_{0,0} \\
%     \alpha_{1,0} \\
%     \alpha_{0,1} \\
%     \alpha_{2,0} \\
%     \alpha_{1,1} \\
%     \alpha_{0,2} \\
%     \hdots \\
%     \alpha_{0,n}
%   \end{pmatrix} =
%   \begin{pmatrix}
%     1 \\ 1 \\ 1 \\ \vdots \\ 1
%   \end{pmatrix}
% \end{equation*}

% If we solve the above system, we arrive at the fact that $\alpha_{0,0}=1$ and all other $\alpha_{i,j}=0$ --- at least if all columns within the huge matrix are linearly independent (which is the case if all support points are pairwise different).

% Thus, we can conclude equation (\ref{eq:sum-basis-functions-is-1}).

\newpage
\part{Comparing the exact and approximate solution of the Lax-Fried\-richs flux}
\label{part:polynomial-comparison}

In the next sections, we will examine an aspect of wave propagation between two triangles, namely the quality of the approximation employed by the Lax-Friedrichs flux.

While we might be able to evaluate the Lax-Friedrichs flux exactly, the involved computations are long and complex and can result in infeasibly long computation times.

On the other hand, approximations through various numerical methods can speed this process up significantly. We analyze what kind of accuracy loss that implies and if it is a suitable method for realistic computation.

We have two adjacent triangles called $R$ and $L$ respectively, sharing a common edge $E$. Each triangle has its own flux function for every component (height $h$ and momentum $u$), evaluated over coordinates on the triangle. While the original problem arises in two dimensions, we can reduce it to one dimension by parametrizing the relevant coordinates.

After parametrization along the edge $E$, this results in a function $[0,1]\rightarrow\reals$ for each component and each triangle, resulting in four functions (see section \ref{sec:original-situation-2D problem} for more detailed information regarding the parametrization).

To approximate the result numerically, we will take a number of sample points along the edge and evaluate the flux functions at those places. This is the prevalent method employed in this field \cite{castro07high-order-ader-fv-dg-numerical-methods,schwaiger08adaptive} and is also used by the Sierpinski framework. Recall from section \ref{sec:matrix-extraction} that it uses a Gaussian quadrature to compute this integral. This introduces an error, since we have a rational function, for which polynomial Gaussian quadrature may not produce exact results anymore.

To analyze the severity of the error, we will compare the two resulting terms in the form of the $L^1$-norm.

\section{Numerical flux}
\label{sec:numerical-flux}

The flux is a physical quantity that describes the exchange of information between adjacent cells (e.g. in Discontinuous Galerkin and Finite Volume methods).

We compute the numerical flux by sampling the edge at certain points. There are several possibilities to compute the numerical flux, and we focus on one of them: the Lax-Friedrichs flux.

\subsection{Flux function}
\label{sec:flux-function-intro}

We will see in section \ref{sec:original-situation-2D problem} that -- even if we have originally a two-dimensional problem -- we can parametrize it in a way such that we can consider a one-dimensional problem. If we do so, we have to consider the flux function in one dimension, as well. Hence, we take the following as flux function, which is the same problem as started in section \ref{sec:shallow-water-equations} of the introduction, only mapped to one dimension \cite{george2004numerical}:

\begin{equation}
  \label{eq:flux-function-definition}
  F\left(
    \begin{pmatrix}
      h \\ u
    \end{pmatrix}
  \right) =
  \begin{pmatrix}
    u \\
    \frac{u^2}{h} + \frac{1}{2} g h^2
  \end{pmatrix}
\end{equation}

\subsection{Lax-Friedrichs flux}
\label{sec:lax-friedrich-definition}

The Lax-Friedrichs flux is defined as follows \cite{leveque2002finite}:
\begin{equation}
  \label{eq:lax-friedrich-definition}
  F_{LF}(p^R,p^L) = \dfrac{1}{2}\cdot (F(p^R) + F(p^L)) - \alpha \cdot (p^R - p^L)
\end{equation}

In (\ref{eq:lax-friedrich-definition}), $p^R$ and $p^L$ stand for the height and velocity along the right and the left edge, respectively. The term $F(p)$ is defined as in equation (\ref{eq:flux-function-definition}), and $p$ is a two-component vector. $p^R$ and $p^L$ are polynomials depending on a variable ($x$ in our one dimensional case). These polynomials are containing two components (for height and momentum). $p$ is technically a function $[0,1]\rightarrow\reals$, thus we should write $p\left(x\right)$ and $F\left(p\left(x\right)\right)$. For simplicity, we will simply use $p$ and $F\left(p\right)$ respectively, but we need to remember that fact when we differentiate or integrate over $x$, because we cannot treat $p$ and $F\left(p\right)$ as constants.

The factor $\alpha$ is used to regulate the quality of the resulting term.
Lower values result in increased accuracy but reduces the stability of the algorithm.
Depending on simulation circumstances, $\alpha$ is either set globally and treated the same for all triangles at a low enough value to guarantee stable computation, or is determined based on the quantities stored for certain triangles at run time.
The various methods and their properties are discussed in \cite{leveque2002finite}.
For our purposes, we will leave it invariant in time and space across the whole domain.
We refer to \cite{cockburn1999discontinuous,leveque2002finite} for readers interested in this issue and other choices of flux functions.

Depending on how $\alpha$ is handled, it results in different methods. Sometimes it is set globally and treated the same for all triangles, other times it is determined based on the qualities of certain triangles at run time, which obviously results in higher computation time. The various methods and their properties are discussed in \cite{leveque2002finite}. For our purposes, we will leave it invariant in time and space across the whole domain.

\section{Deriving the error term}
\label{sec:deriving-error-term-i}

\subsection{Parametrizing a 2D problem}
\label{sec:original-situation-2D problem}

The original problem arises when two adjacent triangles share one edge.
Each triangle has several quadrature points which are interpolated to obtain a polynomial for the corresponding triangle.

Figure \ref{fig:two-triangles-and-some-support-points} shows two adjacent triangles and exemplary positions of the quadrature points.
In the case shown we would use six integration points, resulting in a two-variable polynomial of degree 2. We could use more integration points, thereby increasing the degree of the resulting polynomial.

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}[scale=2]
    \begin{scope}[xshift=-.1em,yshift=-.1em]
      \draw (0,0) -- (0,1) -- (1,0) -- (0,0);
      %\draw[fill=black] (0,0) circle (0.02);
      %\draw[fill=black] (0.5,0) circle (0.02);
      \draw[fill=black] (0.79,0.21) circle (0.02);
      \draw[fill=black] (0.5,0.5) circle (0.02);
      \draw[fill=black] (0.21,0.79) circle (0.02);
      %\draw[fill=black] (0,0.5) circle (0.02);
    \end{scope}
    \draw[->,thick] (0,1.075) -- node[right]{$\lambda$} (1.075,0);
    \begin{scope}[xshift=1.15cm,yshift=1.15cm,rotate=180]
      \draw[fill=black] (0.79,0.21) circle (0.02);
      \draw[fill=black] (0.5,0.5) circle (0.02);
      \draw[fill=black] (0.21,0.79) circle (0.02);
      \draw (0,0) -- (0,1) -- (1,0) -- (0,0);
    \end{scope}
  \end{tikzpicture}
  \caption{Two adjacent triangles with quadrature points}
  \label{fig:two-triangles-and-some-support-points}
\end{figure}

Since we are interested in the situation along the adjacent edges, which is a linear curve, we can remodel the problem to one dimension, which will reduce the computational complexity while maintaining the same accuracy. This means that we do not actually construct two-variable functions for the whole triangle, but instead parametrize it with one variable $\lambda \in [0,1]$. The variable $\lambda$ can be thought of simultaneously traversing both adjacent edges (see figure \ref{fig:two-triangles-and-some-support-points}).

\subsection{Constructing the polynomials}
\label{sec:constructing-polynomials}

As stated in the introduction, we have to construct $p^L$ and $p^R$, the polynomials describing the height and momentum of the right and left triangle. We construct these polynomials by interpolating certain points.

We do so for the left and the right triangle, and for the height and the momentum. We call the points we have chosen the \emph{support points} (or integration points) for the left and right triangle.
For convenience, we have decided to use a naming convention for these points.
Assume that we have $n$ points for each triangle.
We denote the components of the points of the left triangle
\begin{equation*}
\left(x_1,\begin{pmatrix}
    h_1^R \\ u_1^R
  \end{pmatrix}\right), \dots , \left(x_n, \begin{pmatrix}
    h_n^R \\ u_n^R
  \end{pmatrix}\right)
\end{equation*}
and the points for the right triangle
\begin{equation*}
\left(x_1,\begin{pmatrix}
    h_1^L \\ u_1^L
  \end{pmatrix}\right), \dots , \left(x_n,\begin{pmatrix}
    h_n^L \\ u_n^L
  \end{pmatrix}\right),
\end{equation*}
i.e.\,we introduce a superscript indicating whether we are dealing with points within the left or the right triangle.

As we can see, each point consists of two components, first a position on the edge ($x_i, i \in \{1 \dots n\}$) and second a vector with the corresponding height and momentum, ($h_i^R$ and $u_i^R$ for the left triangle and $h_i^L$ and $u_i^L$ for the right, with $i \in \{1 \dots n\}$).

Intuitively one would think the support points should be equidistant to each other, but other distributions yield better results.
Eventually we will need to integrate over the resulting function, which is numerically expensive. To compute it efficiently, we will employ the Gaussian quadrature for polynomials. This is common practice according to \cite{schwaiger08adaptive,castro07high-order-ader-fv-dg-numerical-methods,boeck08discontinuous-galerkin-verfahren}.

We examine two settings for our support points that are used to construct the polynomials:

\begin{description}
\item[Support points according to Gaussian quadrature] Since we eventually need to integrate over the edge (using Gaussian quadrature), it is convenient to choose the same integration points to construct the polynomials. So we choose our integration points to coincide with them for easier computation. The corresponding results for this choice are summed up in section \ref{sec:results}.
\item[Equidistant support points] However, we also investigated the situation for an equidistant distribution of support points. We show the corresponding results, as well as a short explanation of why this setting may be useful, in section \ref{sec:equidistant-distribution-of-support-points}.
\end{description}

To construct the polynomials assume we have a method $interpolate\left(\{\left(x_i,y_i\right) \mid i \in \{1 \dots n\}\}\right)$.
This method takes a set of points $\left(x_1,y_1\right),\dots,\left(x_n,y_n\right)$ and returns the polynomial of degree $n-1$ that goes through all the points within the set.
\footnote{In practice this can be e.g.\,a Lagrange interpolation routine. Maple provides the function \texttt{CurveFitting[PolynomialInterpolation]} for that purpose.}
Here we introduce a notational shorthand for this construct by writing $interpolate\left(x,y\right)$ for $interpolate\left(\{\left(x_i,y_i\right) \mid i \in \{1 \dots n\}\}\right)$.

We then generate four polynomials as follows:

\begin{itemize}
\item $p^L_h(x) := interpolate (x,h^L)$. Polynomial interpolating the height values for the left triangle.
\item $p^L_u(x) := interpolate (x,u^L)$. Polynomial interpolating the momentum values for the left triangle.
\item $p^R_h(x) := interpolate (x,h^R)$. Polynomial interpolating the height values for the right triangle.
\item $p^R_u(x) := interpolate (x,u^R)$. Polynomial interpolating the momentum values for the right triangle.
\end{itemize}

We then combine two polynomials into one to obtain the following:

\begin{equation*}
  p^L(x) :=
  \begin{pmatrix}
    p^L_h(x) \\ p^L_u(x)
  \end{pmatrix}, \quad
  p^R(x) :=
  \begin{pmatrix}
    p^R_h(x) \\ p^R_u(x)
  \end{pmatrix}
\end{equation*}

As was mentioned in section \ref{sec:flux-function-intro} $p^L$ and $p^R$ are actually functions that map $x \in [0,1]$ onto a vector containing the values of the height and the momentum polynomial for the respective triangle, we still call $p^L$ and $p^R$ polynomials to refer to the resulting functions, which we will be working with.

\subsection{Computing the error introduced by the quadrature}
\label{sec:goal-intro}

We first analyze the difference between the following two ways to compute the flux based updates:

\begin{description}
\item[Exact solution] We apply the Lax-Friedrichs flux directly to the polynomials $p^L$ and $p^R$ (depending on $x$), and compute its exact value. The resulting values are rational functions depending on $x$. We call these functions $N\left(x\right)$ (a vector which has a polynomial function for the height as its first component, while the second component is a rational function, representing the momentum).

\item[Approximate solution] We apply the Lax-Friedrichs flux to the support points, and compute the resulting values, i.e.\,we compute the following for $i \in \{1 \dots n\}$:
  \begin{equation*}
    F_{LF}\left(
      \begin{pmatrix}
        h_i^R \\ u_i^R
      \end{pmatrix},
      \begin{pmatrix}
        h_i^L \\ u_i^L
      \end{pmatrix}
    \right) :=
    \begin{pmatrix}
      h_i^F \\ u_i^F
    \end{pmatrix} = F_i
  \end{equation*}

  We introduce shorthand notation $F_i$ for the resulting values. Each $F_i$ has the same structure as the $x_i$, only it is evaluated by the flux function. Next we interpolate the points (i.e.\,we interpolate the first components to obtain one polynomial, and we interpolate the second components to obtain another polynomial). The resulting polynomials are called $N^\prime\left(x\right)$ (interpreted like $N\left(x\right)$ as explained above).

  That is, we set
  \begin{equation*}
    N^\prime\left(x\right) :=
    \begin{pmatrix}
      interpolate\left(x,h^F\right) \\ interpolate\left(x,u^F\right)
    \end{pmatrix}
  \end{equation*}

\end{description}

Our goal will then be to compare $N\left(x\right)$ to $N^\prime\left(x\right)$ (component-wise, of course). We compare the quality of an approximation by considering the $L^1$ norm:

\begin{equation}
  \label{eq:integral-norm-definition}
  I := \int\limits_{x=0}^1 | N\left(x\right) - N^\prime\left(x\right) |\  dx,
\end{equation}
where the integration is to be understood component-wise.

\subsection{Height error analysis}
\label{sec:height-error-p2}

At this point, we can examine the height error and discover something important.
If we recall the Lax-Friedrichs flux function and look at the height component, we see that the flux function at that point only contains $u$:

\begin{equation}
  \label{eq:lax-friedrich-definition}
  F_{LF}(p^R,p^L) = \dfrac{1}{2}\cdot (F(p^R) + F(p^L)) - \alpha \cdot (p^R - p^L)
\end{equation}

This implies something important for the interpolation, namely that the resulting functions will be a polynomial.
More precisely, it will result in a polynomial of degree $n-1$ (as polynomial interpolation for $n$ points always results in polynomials of degree $n-1$).
Since the approximating function $N^\prime\left(x\right)$ is also a polynomial obtained by interpolation of the $n$ support points, it is a polynomial of degree $n-1$ as well.
Both of these terms, however, are the same in exactly $n$ points.
With the fundamental theorem of algebra, we can conclude that both polynomials are the same, which means the error will be exactly 0.

This means we can ignore the resulting error for the height component, as it will be 0 regardless of how we choose the other variables.
That is why, for the rest of this part, we will only display error plots for the momentum component of $I$ and omit the plots for the height error.

\subsection{\texorpdfstring{How to evaluate $I$}{How to evaluate I}}
\label{sec:how-to-eval-I}

Evaluating the integral in equation (\ref{eq:integral-norm-definition}) is not a trivial task.
Even if delegating most of the work to the symbolic math program Maple, we experienced some pitfalls while evaluating the term.

The main challenge with exact integration is that Maple has to compute the zeroes of $\left|N\left(x\right)-N^\prime\left(x\right)\right|$, which can be a time consuming task, depending on the functions used.

There are several possibilities to overcome this problem. The first that may come to mind is to use a Gaussian quadrature (or any equivalent quadrature scheme for rational functions). However, even after extensive research we could not find any suitable quadrature methods for rational functions.

This is why we consider three other possibilities for computing the error term:

\begin{description}
\item[Numerical integration in Maple] Another approach was to use Maple's numerical integration facilities. If we tell Maple to
  \begin{center}
    \texttt{evalf(Int(abs(N(x)-N'(x)),x=0..1))},
  \end{center}
  we get a good result.

  The downside of this method is that is computationally infeasible for a detailed analysis of higher order due to the increasingly complex formulas.
\item[Exact integration between certain points] $N\left(x\right)$ and $N^\prime\left(x\right)$ intersect when supplied the support points, i.e.\,
  \begin{equation*}
    N\left(x_i^F\right) = N^\prime\left(x_i^F\right) \text{ resp. } N\left( x_i^F \right) - N^\prime\left( x_i^F \right) = 0 \quad , \forall i \in \{1 \dots n\}.
  \end{equation*}

  Knowing this, we can derive some zeroes of the function $N(x)-N'(x)$ (but possibly not all). If we compute
  \begin{equation*}
    \sum\limits_{i=1}^{n-1} \left|\ \int\limits_{x=x_i}^{x_{i+1}} N\left(x\right) - N^\prime\left(x\right)\ dx \right|,
  \end{equation*}
  we get a more accurate result as an approximation for $I$ compared to simply computing the integral over $[0,1]$.

  If we now divide each interval $[x_i,x_{i+1}]$ into several smaller intervals, and simply integrate over them, we can reduce the error ad libitum.

  While we can tweak this method by simply evaluating more intervals, this comes at the price of larger computation time.
\item[Numerical quadrature with rectangle method] The last approach we tried was to divide the area under the function
  \begin{equation*}
    \left| N\left(x\right)-N^\prime\left(x\right) \right|
  \end{equation*}
  into several stripes, and compute the area of these stripes. The more stripes we have, the more accurate our result will be, at the expense of computation time.
\end{description}

It is just as well possible to use the first or second approach, although this requires to tell Maple that it should first make all necessary substitutions, and then plot the resulting graphs. This is not only harder to accomplish but also results in a mess of code, but would not produce more accurate results, which is why we did not explore these options further. So we eventually went for the last of these approaches using the rectangle method for quadrature. We experienced quite good results using this method.

We eventually implemented our own tool to accomplish that task.
Some information about it will be provided in section \ref{sec:making-the-whole-thing-interactive}.

\subsection{Singularities}
\label{sec:singularities}

If we take a look at the second component of $I$, we can see that it contains a fraction (this is due to the flux function \ref{eqn:shallow-water-flux}, which contained a fraction itself). This means that there could be some singularities involved. If we integrate, we need to pay attention to these singularities, although we will see that they will have no practical relevance for us.

If we use two support points for each edge (all points except $p_1^L$ are $(10,0)$) and expand the term $N\left(x\right)-N^\prime\left(x\right)$, we can clearly see a fraction in its second component, i.e.\,the term describing the error in the momentum component. The fraction term in this example looks as follows (numerator omitted for simplicity):

\begin{equation*}
  \frac{1}{h_1\cdot(1.732 h_1 \cdot x - 17.320 x - 1.366 h_1 + 3.660)}
\end{equation*}

Remember from equation (\ref{eq:integral-norm-definition}) that the term $I$ integrates over $N\left(x\right)-N^\prime\left(x\right)$ from $x=0$ to $x=1$. Thus -- depending on the value of $h_1$ -- we run the risk that there is a singularity (w.r.t.\,the variable $x$). We can compute that we have a singularity at
\begin{equation*}
  x_0=\frac{683\, h_1 - 1830}{866\, h_1 - 8600}.
\end{equation*}

Since we take the integral between 0 and 1, it is of particular interest if the value $x_0$ can be within the interval $\left[ 0,1 \right]$, since in this case we would integrate over a singularity where the integral is not (or at least might not be) properly defined. If it is a pole, which is likely in this situation, we would obtain extremely high values from the numerical integration over that domain.

We obtain that $x_0 \in \left[ 0,1 \right]$ only if $h_1 \leq 2.679$ or $h_1 \geq 37.322$. So we see that for this particular case we have singularities only for values of $h_1$ that differ greatly from all other values for the height (that are in this case set to 10).

We tried these computations for other scenarios, including a variation in support points leading to similar results.
%The results were always the same, that the height would have to differ in absurd levels to even approach the required region of the singularity. %gestrichen auf Martins Vorschlag

However, it is still important to note that these singularities do in fact occur. While they did not interfere with our computations, similar application with largely differing sample values may encounter a challenge at this point.

Analytically deriving a general formula for the occurrence of such singularities is a difficult task due to the large number of variables. This is further complicated by the fact that the number of variables itself is not static but depends on the number of support points. We even encountered problems computing the exact $x$ values for the singularities for four support points, due to the complexity of the term, although they could still be found visually once the function was evaluated and plotted, coinciding with our previously computed results.

\newcommand{\fracsumme}{\mathtt{approx\_int}}

% It is convenient to research this fraction and look what happens if we set $h_1$ approximately to 1.6. Let us -- for simplicity -- assume that we numerically approximate this component of $I$ using five stripes (i.e.\,we take the function values at $x\in\left\{ 0, 0.25, 0.5, 0.75, 1 \right\}$). So, we can consider $\fracsumme(h_1,u_1) := \sum_{i=0}^4 \badfrac(\frac{i}{4})$ which is a part of what is plotted in figure \ref{fig:two-points-p1-wider-range}.

%If we evaluate this for corresponding values for $h_1$ and $u_1$ (i.e.\,in particular for $h_1\approx 1.6$ and $h_1\approx 0$), we obtain results summed up in table

% \begin{table}[ht]
%   \centering
%   \begin{tabular}[ht]{ll}
%     \hline
%     $\fracsumme(0,0)$ & -2.6 \\
%     $\fracsumme(0,1)$ & -11.3 \\
%     $\fracsumme(0,0)$ & -27.0 \\
%     $\fracsumme(0,0)$ & -49.7 \\
%     $\fracsumme(1.6,0)$ & -0.87 \\
%     $\fracsumme(1.6,1)$ & -2.52 \\
%     $\fracsumme(1.6,2)$ & -13.9 \\
%     $\fracsumme(1.6,3)$ & -34.9 \\
%   \end{tabular}
%   \caption{Selected values for the term $\fracsumme$ depending on $h_1$ and $u_1$}
%   \label{tab:selected-values-for-fracsumme}
% \end{table}

\section{\texorpdfstring{Visualizing the error term $I$}{Visualizing the error term I}}
\label{sec:making-the-whole-thing-interactive}

We are now going to plot the momentum component of the term $I$ as described in equation (\ref{eq:integral-norm-definition}).
First, we have to devise a strategy to compactly but efficiently visualize such a complicated term.
Afterwards, we present our tool, developed to simplify plotting as much as possible in order to offer more insight to the quadrature errors.

\subsection{Visualization introduction}
\label{sec:a-word-on-visualization}

\subsubsection{Functions with parameters}

We now consider a generalized function of same order, namely

\begin{equation*}
  y = a \cdot x + 1.
\end{equation*}

This equation describes a line going through point $(0,1)$ with varying gradient, namely $a$. That means we actually have a whole collection of lines given by this equation. If one wants to plot the function $y = a \cdot x + 1$, there are essentially two possibilities:

\begin{itemize}
\item Focus on specific values of $a$ and plot $y = a \cdot x + 1$ for these values.
\item Generate a 3D plot for varying values of $a$ and $x$. This plot has then two axes for $a$ and $x$ spanning two dimensions, the resulting value $y$ is then denoted by the last dimension.

  Each point contained in the graph of this 3D plot has three coordinates $(x,a,y)$, and these three coordinates have to satisfy the condition $y = a \cdot x + 1$.
\end{itemize}

We compare the two methods in figure \ref{fig:simple-parametric-functions}. Note that we can interpret the lines in the two dimensional plot as slices from the three dimensional plot.

\begin{figure}[ht]
  \centering
  \subfigure[Choosing particular values for $a$]{
    \includegraphics[scale=0.5]{simple_parametric_2d.pdf}
  }
  \subfigure[A three dimensional plot]{
    \includegraphics[scale=0.5]{simple_parametric_3d.pdf}
  }
  \caption{Plotting simple parametric functions using different approaches.}
  \label{fig:simple-parametric-functions}
\end{figure}

Now, we extend the above function by another variable, obtaining $y = a \cdot x + b$.
If we want to plot this collection of lines, our options are similar to above.
The problem now is that we cannot plot all our variables at once, essentially giving us these choices:

\begin{itemize}
\item Focus on specific values for $a$ and $b$ and generate several line plots for those values with the variable $x$.
\item Focus on specific values for $a$, and generate several 3D plots for those values with the variables $x$ and $b$.
\item Similar to the previous possibility, focus on specific values for $b$ (instead of $a$) and generate several 3D plots for those values with the variables $x$ and $a$.
\end{itemize}

As the number of parameters increases, the number of possibilities for plotting grows with it.
Since we can have a large number of variables, this is what we will do as well, leave one or two variables variable, while fixing the remaining variables and plotting the one-dimensional or two-dimensional result respectively.

\subsection{\texorpdfstring{Interpreting $I$ as a function of support points}{Interpreting I as a function of support points}}
\label{sec:some-considerations}

Recall from section \ref{sec:constructing-polynomials}, that the terms $p^L$ and $p^R$ depend on the chosen support points. Moreover, $N\left(x\right)$ depends on these polynomials. Thus, the term $N\left(x\right)-N^\prime\left(x\right)$ also depends on the polynomials and by extension on the support points. This means, we have $4n$ variables\footnote{We have $n$ points for each of the two adjacent edges, each point consisting of a height and a momentum component, resulting in $4n$ variables.} in this term.
This implies that $I$ (see equation (\ref{eq:integral-norm-definition})) also depends on the single points
$( h_1^L , \dots, u_1^L)^T ,\dots, ( h_n^L , \dots, u_n^L)^T$ and $( h_1^R , \dots, u_1^R)^T ,\dots,( h_n^R , \dots, u_n^R)^T $.
For plotting, the amount of variables requires us to later mask out some of them to be able to obtain useful plots.

We could interpret $I$ as a function over these $4n$ values onto a two-component vector containing the integral from the $h$ and $u$ components. If we were precise, we would then write it in function notation $
I\left(
  ( h_1^L , \dots, u_1^L)^T ,\dots,
( h_n^L , \dots, u_n^L)^T ,
( h_1^R , \dots, u_1^R)^T ,\dots,
( h_n^R , \dots, u_n^R)^T \right)$ instead of simply $I$. However, most of the time we prefer $I$, since we are interested in notational simplicity.

We decided to use 3D plots, where we have two axes representing two of the $4n$ variables. All the other variables are fixed to certain values.
% Later we will explain how we can visualize $I$. Since there are many variables to choose, we decided to fix all but one point and visualize the result.
For example, if we fixed all points to $( 10 , \dots, 0)^T$ except the first point, this would mean we are interested in
\begin{equation*}
  I\left(
    \begin{pmatrix}
      h_1^L \\ u_1^L
    \end{pmatrix},
    \begin{pmatrix}
      10 \\ 0
    \end{pmatrix}, \dots,
    \begin{pmatrix}
      10 \\ 0
    \end{pmatrix}
  \right).
\end{equation*}

We can now use a 3D plot with the $x$ axis representing $h_1^L$ and the $y$ axis representing $u_1^L$. The $z$ value would then represent the resulting value of $I$.

\subsection{Plotting tool}
\label{sec:plotting-tool-intro}

To plot our results we considered several options. Maple supports dynamic plotting to a certain degree, and since we used it for some of our previous symbolic and numeric computation, we tried employing it to obtain the plots we need. Essentially, one can obtain a window with movable sliders that change a plot adjusting to it.

However, we realized that this method had some downsides. We are, in principle, interested in plotting several graphs simultaneously (this will be relevant to us in the next part). Showing several plots simultaneously affected by the same sliders proved to be a challenge, and was not doable with the native plotting services within Maple due to limited customization.

% It may have been possible to write a custom Maplet (interactive Maple applet) which did what we wanted, but that turned out to be more complex than an implementaion from the ground up
% cumbersome due to Maple's limited applet functionality.\todo{Ihr koenntet das "wissenschaftlicher" formulieren ala "Due to restrictions of Maples functionality regarding applet programming, our requirements were not satisfied". Oder so.}

Moreover, we wanted to be able to quickly switch the coordinate axes, so that we can compare plots with each other in real time, which also did not work the native implementation of Maple. Finally, the user interface of Maple is not intended to be used with that many variables, which posed a challenge for our $4n$ variables in $I$. The only alternative would have been to cut some of them out, which would make the tool less interactive.

So we decided to develop an own tool that helps us visualizing our data properly.

We implemented a program that is capable of doing the following:

\begin{itemize}
\item Read any number of functions from a file. In our case these functions will be the components of $I$, which are the contents of $N\left(x\right)-N^\prime\left(x\right)$ (i.e.\,the integrand within the integral of $I$). This file is generated by Maple in our case, since we use it for most of our computations, but it can be created manually by the user as well.
\item Extract variable names from the mathematical expressions extracted from the file (in our case, this will be mainly the variables $u_i$ and $h_i$ in a suitable string representation\footnote{We decided to use a simple scheme for the internal variable names: $u_1^L$ becomes \texttt{u\_1} and $u_1^R$ becomes \texttt{U\_1}, while $h_3^L$ becomes \texttt{h\_3} and $h_4^R$ becomes \texttt{H\_4}.}). The variable $x$ is handled separately since this is the variable that we want to integrate over to compute $I$.
\item Offer a way to dynamically change the values of the variables (i.e.\,the $u_i$ and $h_i$). This is achieved by a slider for each variable (see figure \ref{subfig:plotting-tool-variables}).
\item Numerically evaluate $I$ for the given functions. This is achieved using the technique described in section \ref{sec:how-to-eval-I}.
\item Choose any of the variables to be used as coordinate axes for a 2D or 3D plot.
\item Plot $I$ using \texttt{gnuplot}.
\item Export plots and generate a proper TeX-file.
\end{itemize}

The implementation turned out to be very useful and easier to maintain and handle than any equivalent Maple solution. Figure \ref{fig:plotting-tool} shows what the tool looks like\footnote{Since the program undergoes (sometimes heavy) changes, the screen shot shown in figure \ref{fig:plotting-tool} might not show the most up-to-date version of our tool.}.

\begin{figure}[ht]
  \centering
  \subfigure[Adjusting variables and axes]{
    \label{subfig:plotting-tool-variables}
    \includegraphics[scale=0.5]{simpleplotter_screen1.png}
  } \hspace{.3cm}
  \subfigure[Gnuplot settings/Settings for numerical integration]{
    \label{subfig:plotting-tool-gnuplot}
    \includegraphics[scale=0.5]{simpleplotter_screen2.png}
  }
  \caption{Plotting tool window.}
  \label{fig:plotting-tool}
\end{figure}

Figure \ref{subfig:plotting-tool-variables} shows the page that is used to adjust the variables. As we can see, the tool has detected variables \texttt{u\_1}, \texttt{h\_1}, \texttt{U\_1}, and so on. Moreover we can see that e.g.\, the parameter \texttt{u\_2} is set to a value of 3.2, while \texttt{u\_1} is set to 0.0.

The radio buttons determine which variables are used as axes. In figure \ref{subfig:plotting-tool-variables}, in the first option row, the column for \texttt{u\_1} is selected, while in the second row, we chose \texttt{h\_1}. This means that the plotter uses \texttt{u\_1} as $x$ axis, and \texttt{h\_1} as $y$ axis for the 3D plot.

Configure \ref{subfig:plotting-tool-gnuplot} shows exemplary settings for plotting. ``Samples'' and ``Isosamples'' are gnuplot-specific parameters that determine how fine-grained gnuplot is plotting the curve. A hundred samples and ten isosamples have shown to be a fitting balance between an accurate graph and an acceptable computation time. The settings for lower and upper $x$ and $y$ values determine the range that is plotted by gnuplot.

The parameter ``stripes'' determines how many stripes are used to numerically evaluate the integral $I$. After trying some values we found out that using less than five stripes is noticeably inaccurate. However, using more than five stripes does not significantly affect the result.

\subsection{How to interpret the graphs}
\label{sec:how-to-interpret-graphs}

All the plots in this document are generated using our tool. We said that our tool reads two functions from a file and interprets them as the two components of $N\left(x\right)-N^\prime\left(x\right)$. The tool generates a plot representing the error in the $u$ component (momentum).

To understand how our plots work, we can take a look at figure \ref{fig:two-points-all-the-same}. In this example, we used 2 points on each triangle and edge. That is, we have four (relevant) points in total, two of which we have plotted here. This is why figure \ref{fig:two-points-all-the-same} has two sub-figures.

Looking at sub-figure \ref{subfig:two-points-p1-height-momentum}, its caption says that point $p_1^L$ is varying. To be precise, its height and momentum vary along the plot axes.

The first part of this caption tells us that we are fixing all points except $p_1^L$ to a specific value (in this case it was $\left(10, 0\right)^T$), so we are considering the term 

\begin{equation*}
  I\left(\left(10, 0\right)^T, \left(h_2^R, u_2^R\right)^T, \left(10, 0\right)^T, \left(10, 0\right)^T\right).
\end{equation*}


% In subfigure \ref{subfig:fixing-p2-in-first-example}, we deal with a function that has two variables (namely $h_2^R$ and $u_2^R$). Thus, we can create a 3D plot showing this function. Remember that $I$ was a \emph{vector} containing two components. This is why subfigure \ref{subfig:fixing-p2-in-first-example} contains two plots. The left plot stands for the $h$ component, while the right one represents the $u$ component.

The orientation of the axes is displayed in figure \ref{fig:orientation-of-axes}.

\begin{figure}[th]
  \centering
  \begin{tikzpicture}
    \draw[->,semithick] (0,0) -- (0,1) node[above]{Value of $I$}; % z
    \draw[->,semithick] (0,0) -- +(-30:1) node[below right]{$u$}; % x
    \draw[->,semithick] (0,0) -- +(210:1) node[below left]{$h$}; % y
  \end{tikzpicture}
  \caption{Orientation of the axes used for our 3D plots}
  \label{fig:orientation-of-axes}
\end{figure}

To plot $u$ and $h$ we need to know their respective ranges as well. This is noted in the caption of the whole figure. For example, in figure \ref{fig:two-points-all-the-same}, the values for $u$ are in $[8, 12]$ and the values for $h$ in $[-4,4]$.

% Looking back at the caption of the subfigure, which had ``1/0.04'' in the latter part (subfigure \ref{subfig:fixing-p2-in-first-example}). This tells us the range of the values of $I$ (i.e.\,the ``$z$ axis''). To be precise, the firs part tells us that the maximal plotted value for the h component is 1, while the maximal $u$ component value is 0.04. A single number indicates a range between 0 and that number, while an explicit range indicates the given range (for an example, see \ref{subfig:example-with-range}, where the $h$ component plot range goes from 0.25 to 0.55, and the $u$ component from 0 to 0.03).

Since we are visualizing norms (in the form of integrals over absolute error terms) and not actual physical quantities, we cannot make quantitative observations as to whether results are good or bad. Hence, when comparing two results we cannot determine how much one is better than the other. However, we can determine which result is better. And since the calculated values are error terms, the smaller the error term the better the result.

Additionally, because we are using a norm there is no physical unit with which the error term could be meaningfully expressed. It could be considered a ``normalized water height'' or ``normalized momentum'', respectively. However, that would not contribute to our understanding of the subject matter and doesn't describe any physical property directly, so we omit units in this place and in the plots. As mentioned before, we can only compare two normalized errors and determine which one is the better.

\section{Distribution of support points according to Gaussian quadrature}
\label{sec:results}

The support points for this section can be extracted from table \ref{tab:x-coordinates-gauss-quadrature}.

\subsection{Setting all support points to the same value}
\label{sec:setting-all-support-points-to-the-same-value}

First we set all support points to one single value ensuring homogeneous height and momentum. Then, we tackle each point individually and let their values range over a certain domain.

\input{2_punkte_alles_10_0/tex.tex}

Figure \ref{fig:two-points-all-the-same} shows the case for two points on each adjacent edge. Each figure shows what happens if we fix all but one specific support point. For example, figure \ref{subfig:fixing-p2-in-first-example} shows constant values of all support points except $p_2^L$ (containing the variables $h_2^L$ and $u_2^L$) and variable $h_2^L$ in the range from 8 to 12 and $u_2^L$ from $-4$ to 4.
The left part of figure \ref{subfig:fixing-p2-in-first-example} shows the error in the $h$ component\footnote{Computed as described in equation (\ref{eq:integral-norm-definition}).}, while the right half depicts the error in the $u$ component.
The error in the $h$ component is always zero in the range depicted here (about $10^{-15}$ to be precise, which can be interpreted as numerical error). We will generally omit plots that show no error similar to that.

As the plot shows, the error for the $h$ component ranges up to 2.5.

We did the same thing for three support points along each edge (and it can be done for an arbitrary number of support points). We can see the results in figure \ref{fig:three-points-equal}. In general it seems that the structure of the error plots is quite similar across different settings, the error being similar to the ones seen in figure \ref{fig:two-points-all-the-same}.

\input{3_punkte_gleich/tex.tex}

\subsection{Singularities}
\label{sec:plots-discontinuities}

When we look at figure \ref{subfig:fixing-p2-in-first-example}, we see that the error in the momentum component looks somewhat like a parabola with respect to the variable $h_1$. We see a smooth curve.

Remember that we saw in section \ref{sec:singularities} that there might be singularities. We will now see a plot that exposes these singularities to us.

\input{2_points_def_luecke/tex.tex}

Figure \ref{fig:two-points-p1-wider-range} shows a plot that was created using two points per edge, each point having coordinates $(10,0)$. We use the coordinates of $p_1^L$ as axes (i.e.\,$h_1$ and $u_1$). But the axes of this plot now have a wider range than the ones in previous plots. As we can see there, the error drastically grows to about 200, much larger than the one we have seen before. We realize that these discontinuities arise if $h_1$ takes a values of about 1.6 and 0. In fact, there are even more discontinuities that can not be seen here since they occur at more distant values for $h_1$.

\subsection{One point variation}
\label{sec:one-point-variation}

So far we saw what the plots look like if all points have the same value. We are now going to inspect what happens if the single points have different values.

\subsubsection{\texorpdfstring{Decreasing the height of $p_1^L$}{Decreasing the height of p1L}}
\label{sec:decreasing-height-p1}

Now we alter the previously described experiment in one detail. We fix all points as before, but we alter one single point slightly. We do this in order to find out if specific points might have more impact than others.

We start off by using three points per edge, setting each point to $\left(10, 0\right)$ except $p_1^L$ which is set to $\left(9, 0\right)$ (i.e.\,we decrease the height component of $p_1^L$).

\input{3_punkte_1_geschwindigkeit_verringert/tex.tex}

We see the results of this experiment in figure \ref{fig:three-points-h1-}.

Comparing figure \ref{fig:three-points-equal} and \ref{fig:three-points-h1-} it is worth noting that especially the plots for $p_2^L$, $p_3^L$ and $p_3^R$ differ strongly. Additionally, it can be seen that the structure differs, as well as the range of the error.

\subsubsection{\texorpdfstring{Decreasing the momentum of $p_1^L$}{Decreasing the momentum of p1L}}
\label{sec:decreasing-momentum-p1}

While we changed the height in subsection \ref{sec:decreasing-height-p1}, we are now going to change the momentum. The results are shown in figure \ref{fig:three-points-u1-}

\input{3_punkte_1_impuls_verringert/tex.tex}

\subsubsection{\texorpdfstring{Decreasing the height of $p_2^L$}{Decreasing the height of p2L}}
\label{sec:decreasing-height-p2}

Now we are going to examine if it makes a difference when we decrease the height of another point, namely $p_2^L$. We show the results in figure \ref{fig:three-points-h2-}. In this case, the graphs for points $p_1^L$ and $p_3^L$ in particular are noteworthy.

\input{3_punkte_2_hoehe_verringert/tex.tex}

\subsubsection{\texorpdfstring{Decreasing the momentum of $p_2^L$}{Decreasing the momentum of p2L}}
\label{sec:decreasing-momentum-of-p2}

Decreasing the momentum of point $p_2^L$ results in the plots depicted in figure \ref{fig:three-points-u2-}. In particular, the graphs for $p_1^L$ and $p_3^L$ show significant structural differences.

\input{3_punkte_2_impuls_verringert/tex.tex}

\subsection{Random values}
\label{sec:random-values}

The cases we have examined so far were just sample cases to show a qualitative change in error distribution, depending on the variation of certain variables. In reality, variables will differ at several values simultaneously, and are generally not properly aligned. The closest way to simulate that is to assign random values to all the points.

\subsubsection{Two random points}
\label{sec:two-random-points}

We start out with two random points. The results of random assignment of values can be seen in figure \ref{fig:two-points-random}. There are heavy structural differences discernible compared to previous plots.

\input{2_random_new/tex.tex}

\subsubsection{Three random points}
\label{sec:three-random-points}

The same computations for three random points is shown in figure \ref{fig:three-points-random}. Again, it differs strongly from figure \ref{fig:three-points-equal}, where we set all points to the same value.

\input{3_random_new/tex.tex}

\section{Equidistant distribution of support points}
\label{sec:equidistant-distribution-of-support-points}

\subsection{Background}
\label{sec:equidistant-support-points-background}

When one considers the figures depicted in section \ref{sec:results}, it seems intuitive to assume that the error grows if the values for $h$ and $u$ diverge further from the average.

One suspicion that could explain that intuition is due to the functions $N\left(x\right)$ and $N^\prime\left(x\right)$ often diverging towards the borders of $[0,1]$ (i.e.\,$N(x)-N^\prime(x)$ grows towards the borders of $[0,1]$). This was observed by examining several assignments for support points. Even if it is not generally the case, it was often observed.

One way to prevent this is would be to choose the support points such that there are points at 0 and 1 (the borders of $[0,1]$) and the remaining support points are equidistant within this interval. When we choose the points according to this strategy, 0 and 1 are support points and we can be sure that $N\left(x\right)$ and $N^\prime\left(x\right)$ coincide at the values 0 and 1 (i.e.\,$N\left(0\right)=N^\prime\left(0\right)$ and $N\left(1\right)=N^\prime\left(1\right)$). This prevents $N(x)$ and $N^\prime(x)$ from diverging at the border of $[0,1]$ (possibly at the price of increasing the difference within this interval).

We now examine this setting and conduct some experiments. Along the way, note that if we use two or three support points, using equidistant support points corresponds to mapping the support points obtained by the Gaussian quadrature onto the interval $[0,1]$ such that the leftmost resp. rightmost support point becomes 0 resp. 1.

\subsection{Two fixed support points}
\label{sec:equidist-two-fixed}

We start by using two support points along each edge (i.e.\,we choose 0 and 1 as support points). We set both points to $(10,0)^T$ and obtain plots that look similar to the ones that we obtained using the support points of the Gaussian quadrature (i.e.\,$0.5\pm\frac{\sqrt 3}{6}$). Figure \ref{fig:equidist_2_default} compares both variants of support points.

\input{equidist_2_default/tex}

We observe that the error is smaller for equidistant support points than for Gaussian quadrature points. We will now examine if this behavior still occurs if we have three support points.

\subsection{Three fixed support points}
\label{sec:equidistant-three-fixed}

We now consider the same values as in section \ref{sec:setting-all-support-points-to-the-same-value} and set all support points to $(10,0)^T$. We show the resulting plots and the plots for support points from Gaussian quadrature in figure \ref{fig:three-equidistant-all-fixed}.

\input{3_punkte_equidist_alles_gleich/tex.tex}

We can observe from figure \ref{fig:three-equidistant-all-fixed} that now the error is larger for equidistant support points. Still, the shapes are very similar.

\subsection{Three points, variation in one}
\label{sec:equidist-3-one-point-variation}

\subsubsection{Decreasing the height of $p_1^L$}
\label{sec:equidist-3-on-point-var-decr-height-p1}

Now we examine the situation with three support points (at 0, 0.5 and 1), where $p_1^L=(9,0)^T$ and all support points are set to $(10,0)^T$. We show the corresponding plots in figure \ref{fig:equidist_3_alles_default_ausser_p1_9_0}. We restrict ourselves to the errors for $p_2^L$ and $p_3^L$ since the errors for all other points look similar to the error for $p_3^L$.

\input{equidist_3_alles_default_ausser_p1_9_0/tex}

We observe that the shapes of the plots look very similar to the ones obtained from Gaussian support points (see figure \ref{fig:three-points-h1-}). However, the error is significantly larger compared to the error observed with Gaussian support points.

\subsubsection{Decreasing the momentum of $p_1^L$}
\label{sec:equidist-3-on-point-var-decr-momentum-p1}

\input{equidist_3_alles_default_ausser_p1_10_-1/tex}

Similar to before, we can observe the accuracy of equidistant support points falling behind when compared to Gaussian quadrature points.
The results presented in figure \ref{fig:equidist_3_alles_default_ausser_p1_10_-1}.
When compared to figure \ref{fig:three-points-u1-}, the shape can be recognized as the same, but its scale is higher, signifying a larger error.

\clearpage{}

\section{Conclusions}
\label{sec:conslusions}

\paragraph{Behaviour of the error term}

We have seen that the \emph{height} error is zero in the scenarios under consideration, which we derived in section \ref{sec:height-error-p2}. Moreover, the error in momentum shows quite a non-predictable behavior.
%This was expected, since the momentum error is a rational function -- and thus not easy to imagine.
It can vary strongly, both in shape and in scale. When adjusting the height values, it appears to be parabola-shaped, while the momentum values have a minor impact only.

We can (theoretically) make the error arbitrarily large by adjusting the height values such that they differ extremely. This, on the other hand, implies that the error is not bounded.

Furthermore, we saw that it had no significant impact which height or momentum value was differing from the other ones. They all had similar impact, even if the shape was different.

Moreover, we can clearly observe that the closer the height and momentum values of the support points are chosen, the smaller the error gets.
This is good for any practical application, since that is usually the case.
The very idea is to make triangles small enough, to be restricted to a very specific region of the domain.
The smaller the triangle, the more unlikely it is, that the height values differ strongly.
In practice, it is more likely that the relative height difference between points is rather small in comparison. Similarly, the momentum within the triangle does not vary much, because one triangle represents one location within the domain, which is unlikely to move in different directions.

However, trying to determine an exact formula for the deviation is impractical due to the amount of variables that all interact with each other, and may even be impossible.

\paragraph{Singularities}

Additionally, we saw that singularities do not play a significant role in our computations since they only occur for unrealistic scenarios.
This is highly advantageous since we can be sure that the integral exists and quadrature methods give meaningful results.
However, the singularities do exist, even without using any quadrature, as variables appear in the denominator of the momentum flux term.
This may be a problem to consider for extreme situations, such as shock waves, which could potentially result in significant height value differences within one triangle.

\clearpage{}

\part{Stiffness matrix accuracy analysis for SWE}
\label{part:stiffness-matrix}

Recall that we introduced the \emph{stiffness matrix} in section \ref{sec:stiffness-matrix} and explained the basic steps of computing it. We will now in more detail examine how we approximate this matrix and how this approximation deviates from the exact computation. We will then -- similar to part \ref{part:polynomial-comparison} where we considered the Lax-Friedrichs flux -- compare the exact and approximate solution for the stiffness matrix.

\emph{Remark:} To give a short overview of the following mathematical symbols, we refer readers to appendix \ref{sec:cheat-sheet-stiffn} on page \pageref{sec:cheat-sheet-stiffn}.

\section{Sum approximation vs. exact solution}
\label{sec:point-wise-appr-vs-exact-solution-intro}

While deriving the equations necessary for our computations, we introduced several approximations. One of them was the approximation of the combination of various sums, as presented in the computation of the stiffness matrix in section \ref{sec:stiffness-second-line}, in particular in the equations (\ref{eq:third-integral-second-line-1}) and (\ref{eq:third-integral-second-line-2}). For the second line (i.e.\,for the $u_x$ component), we had:

\begin{eqnarray}
  \label{eq:third-integral-second-line-1-analysis-part}
  \int_T F_2(\mathbf{q}) \cdot \nabla \phi \, dT & = &
  \int_T
  \begin{pmatrix}
    \frac{u_x^2}{h} + \frac{1}{2} g h^2 \\ \frac{u_x u_y}{h}
  \end{pmatrix}
  \cdot \nabla \phi_i \, dT \\
  \label{eq:third-integral-second-line-2-analysis-part}
  & \approx &
  \int_T
  \begin{pmatrix}
    \sum_{j=1}^n \left(\frac{u_{x,j}^2}{h_j} + \frac{1}{2} g h_j^2\right) \phi_j \\
    \sum_{j=1}^n \left(\frac{u_{x,j} u_{y,j}}{h_j}\right) \phi_j \\
  \end{pmatrix}
  \cdot
  \begin{pmatrix}
    \pd{\phi_i}{x} \\
    \pd{\phi_i}{y}
  \end{pmatrix} dT \\
  & = & \nonumber \sum_{j=1}^n \left(\frac{u_{x,j}^2}{h_j} + \frac{1}{2} g h_j^2\right) \int_T \phi_j \pd{\phi_i}{x} \, dT \\
  & {} & + \nonumber \sum_{j=1}^n \left(\frac{u_{x,j} u_{y,j}}{h_j}\right) \int_T \phi_j \pd{\phi_i}{y} \, dT
\end{eqnarray}

Remember that we originally approximated the following (see section \ref{sec:matrix-extraction} for further reference):  $(h, u_x, u_y)^T \approx \sum_{i=1}^n (h_i, u_{x,i}, u_{y,i}) \cdot \phi_i(x,y)$. This discretization error is a necessity, since we cannot compute the values for every point on a continuous domain. While we approximate in the above equation as well, the transition from (\ref{eq:third-integral-second-line-1-analysis-part}) to (\ref{eq:third-integral-second-line-2-analysis-part}) uses an approximation of the function by extracting the sum from a product and division. This helps immensely simplify the computations, which is especially desirable at higher order calculations.

As the point-wise sampling approximation $\mathbf{q} = \sum_{j=0}^{n} \mathbf{q}_j \phi$ is not a choice and has to be employed, we will take it as a given and whenever we use the term ``exact'' solution, we mean exact except for the point-wise approximation. The approximation of interest to us is the sum approximation used to extract the sum from within the function call, and this will be subject of analysis in this part.

The equation for the $u_y$ component is analogous:

\begin{eqnarray*}
  \int_T F_3\left(\mathbf{q}\right) \cdot \nabla \phi \, dT & = &
  \int_T
  \begin{pmatrix}
    \frac{u_x u_y}{h} \\ \frac{u_y^2}{h} + \frac{1}{2} g h^2
  \end{pmatrix}
  \cdot \nabla \phi_i \, dT \\
  & \approx & \int_T
  \begin{pmatrix}
    \sum_{j=1}^n \left(\frac{u_{x,j} u_{y,j}}{h_j}\right) \phi_j \\
    \sum_{j=1}^n \left(\frac{u_{y,j}^2}{h_j} + \frac{1}{2} g h_j^2\right) \phi_j \\
  \end{pmatrix}
  \cdot
  \begin{pmatrix}
    \pd{\phi_i}{x} \\
    \pd{\phi_i}{y}
  \end{pmatrix} dT \\
  & = & \sum_{j=1}^n \left(\frac{u_{x,j} u_{y,j}}{h_j}\right) \int_T \phi_j \pd{\phi_i}{x} \, dT \\
  & {} & + \sum_{j=1}^n \left(\frac{u_{y,j}^2}{h_j} + \frac{1}{2} g h_j^2\right) \int_T \phi_j \pd{\phi_i}{y} \, dT
\end{eqnarray*}

We will introduce this notation to refer to the approximated integrand:

\begin{equation}
  \label{eq:point-wise-approx-result-second-line}
  \overline{F}_2(\mathbf{q})
  =
  \begin{pmatrix}
    \sum_{j=1}^n \left(\frac{u_{x,j}^2}{h_j} + \frac{1}{2} g h_j^2\right) \phi_j \\
    \sum_{j=1}^n \left(\frac{u_{x,j} u_{y,j}}{h_j}\right) \phi_j
  \end{pmatrix}
\end{equation}

\begin{equation}
  \label{eq:point-wise-approx-result-third-line}
  \overline{F}_3(\mathbf{q}) \cdot \nabla \phi =
  \begin{pmatrix}
    \sum_{j=1}^n \left(\frac{u_{x,j} u_{y,j}}{h_j}\right) \phi_j \\
    \sum_{j=1}^n \left(\frac{u_{y,j}^2}{h_j} + \frac{1}{2} g h_j^2\right) \phi_j \\
  \end{pmatrix}
  \cdot
  \begin{pmatrix}
    \pd{\phi_i}{x} \\
    \pd{\phi_i}{y}
  \end{pmatrix}
\end{equation}

As mentioned in section \ref{sec:stiffness-second-line}, we approximated the rational function containing a product of sums. This approximation implies some loss of precision. We will now examine the difference between the exact\footnote{Exact in the sense that we substitute $h$ by $\sum_{j=1}^n h_j \phi_j$ and substitute $u_x$ and $u_y$ accordingly.} and our approximate solution and focus on analyzing its validity.

We do so by exemplary examining the second component and compute its exact value (more precisely, we use our original approximation introduced in section \ref{sec:matrix-extraction}):

\begin{eqnarray*}
  \int_T F_2\left(\mathbf{q}\right) \cdot \nabla \phi \, dT & = &
  \int_T
  \begin{pmatrix}
    \frac{u_x^2}{h} + \frac{1}{2} g h^2 \\ \frac{u_x u_y}{h}
  \end{pmatrix}
  \cdot \nabla \phi_i \, dT \\
  & \approx & \int_T
  \begin{pmatrix}
    \frac{(\sum_{j=1}^n \phi_j u_{x,j})^2}{\sum_{j=1}^n \phi_j h_j} + \frac{1}{2} g (\sum_{j=1}^n \phi_j h_j)^2 \\
    \frac{(\sum_{j=1}^n \phi_j u_{x,j}) \cdot (\sum_{j=1}^n \phi_j u_{y,j})}{\sum_{j=1}^n \phi_j h_j}
  \end{pmatrix}
  \cdot \nabla \phi_i \, dT = \\
  & = & \int_T
   \left( \frac{(\sum_{j=1}^n \phi_j u_{x,j})^2}{\sum_{j=1}^n \phi_j h_j} + \frac{1}{2} g (\sum_{j=1}^n \phi_j h_j)^2 \right) \cdot \pd{\phi_i}{x} + \\
  & & \frac{(\sum_{j=1}^n \phi_j u_{x,j}) \cdot (\sum_{j=1}^n \phi_j u_{y,j})}{\sum_{j=1}^n \phi_j h_j} \cdot \pd{\phi_i}{y} \, dT
\end{eqnarray*}

Let us shortly contrast the two different approximation methods. Equations \eqref{eq:contrast-exact-solution} and \eqref{eq:contrast-approx-solution} show the exact and the approximate solution, respectively.

\begin{eqnarray}
  \label{eq:contrast-approx-solution}
  \sum_{j=1}^n \left(\frac{u_{x,j}^2}{h_j}  + \frac{1}{2} g h_j^2\right) \int_T \phi_j \pd{\phi_i}{x} \, dT
  \hspace{-.3cm} & + & \hspace{-.3cm} \sum_{j=1}^n \left(\frac{u_{x,j} u_{y,j}}{h_j}\right) \int_T \phi_j \pd{\phi_i}{y} \, dT \\
  \label{eq:contrast-exact-solution}
  \int_T \left( \frac{(\sum_{j=1}^n \phi_j u_{x,j})^2}{\sum_{j=1}^n \phi_j h_j} + \frac{1}{2} g (\sum_{j=1}^n \phi_j h_j)^2 \right) \pd{\phi_i}{x}
  \hspace{-.3cm} & + & \hspace{-.3cm} \frac{(\sum_{j=1}^n \phi_j u_{x,j}) \cdot (\sum_{j=1}^n \phi_j u_{y,j})}{\sum_{j=1}^n \phi_j h_j} \cdot \pd{\phi_i}{y}\ dT
\end{eqnarray}

Note that equation \eqref{eq:contrast-exact-solution} is significantly more complex than \eqref{eq:contrast-approx-solution}.

These terms (especially equation \eqref{eq:contrast-exact-solution}) become very large and difficult to handle manually, as well as expensive to compute automatically. We will introduce three shorthand notations for the three components of $\mathbf{q}$:

\begin{equation}
  \label{eq:substitutions-for-all-components-with-hat}
  \widehat{h} := \sum_{j=1}^n \phi_j h_j \quad
  \widehat{u}_y := \sum_{j=1}^n \phi_j u_{y,j} \quad
  \widehat{u}_x := \sum_{j=1}^n \phi_j u_{x,j}
\end{equation}

Similarly to the previous steps, we can introduce a hat notation to signal the summation of the individual components, only this time we define it for the entire integrand. In this way, we can simplify the above term:

\begin{equation}
  \label{eq:stiffness-analysis-second-line-exact-approx-simple}
  \widehat{F}_2(\mathbf{q}) \cdot \nabla \phi =
  \left( \frac{\widehat{u}_x^2}{\widehat{h}} + \frac{1}{2} g \widehat{h}^2 \right) \cdot \pd{\phi_i}{x} +
  \frac{\widehat{u}_x \cdot \widehat{u}_y }{\widehat{h}} \cdot \pd{\phi_i}{y}
\end{equation}

Analogously, we define this for the third component:

\begin{equation}
  \label{eq:stiffness-analysis-third-line-exact-approx-simple}
  \widehat{F}_3(\mathbf{q}) \cdot \nabla \phi=
  \frac{\widehat{u}_x \cdot \widehat{u}_y }{\widehat{h}} \cdot \pd{\phi_i}{x} +
  \left( \frac{\widehat{u}_y^2}{\widehat{h}} + \frac{1}{2} g \widehat{h}^2 \right) \cdot \pd{\phi_i}{y}
\end{equation}

\subsection{Notes on the height error}
\label{sec:stiffness-height-error}

We presented sample terms for the second and third components ($u_x$ and $u_y$ respectively), and these are indeed the only terms we will need to take into account for the analysis. The same approximation is applied to the height component, which looks like this:

\begin{eqnarray*}
  F_1(\mathbf{q}) =
  \mathbf{u} =
  \begin{pmatrix}
    u_x \\
    u_y
  \end{pmatrix}
\end{eqnarray*}

In the previous section we discussed that the problem with approximating $F_2(\mathbf{q})$ and $F_3(\mathbf{q})$ were the multiplications and divisions of complicated sum terms. However, $F_1(\mathbf{q})$ now is a linear term as it only depends on the momentum for both the $x$ and $y$ component. Since there is no product or quotient to extract the sum from, it means that the approximation results in the same term again:

\begin{eqnarray}
  \label{eq:stiffness-height-error}
  \overline{F}_1(\mathbf{q}) =
  \begin{pmatrix}
    \sum_{j=0}^{n} u_{x,j} \phi_j \\
    \sum_{j=0}^{n} u_{y,j} \phi_j
  \end{pmatrix} =
  \widehat{F}_1(\mathbf{q})
\end{eqnarray}

This is a very helpful result, as it means that the method is perfectly accurate for one of the three components (disregarding the ubiquitous point-wise approximation). For this reason we will ignore the height error in our analysis and not consider it for future calculations. Instead, we will only look at the error in the second and third components and call them $SE_x$ and $SE_y$ respectively, as those are the $u_x$ and $u_y$ components of the shallow water equations. Note that for future reference that these will occasionally be called just $x$ and $y$, as it is unambiguous that these refer to the respective momentum components.

\subsection{Comparison of exact and approximate solution}

We have now derived the exact solution ($\widehat{F}_2(\mathbf{q}) \cdot \nabla \phi$ and $\widehat{F}_3(\mathbf{q}) \cdot \nabla \phi$) and one that uses an approximation for the rational fraction ($\overline{F}_2(\mathbf{q}) \cdot \nabla \phi$ and $\overline{F}_3(\mathbf{q}) \cdot \nabla \phi$ respectively). We now compare these two with the $L^1$ norm, as we did in the previous part. We chose the $L^1$ norm since it is a widely-used norm for comparing two functions with each other. Moreover, it can be naturally extended from the one dimensional case to the two dimensional case. For that we take the absolute value of the difference, and then integrate over the domain. We do this once for each momentum component, hence we get the two terms $SE_x$ and $SE_y$ respectively:

\begin{eqnarray*}
  SE_x := \int_T \left| \overline{F}_2(\mathbf{q}) \cdot \nabla \phi - \widehat{F}_2(\mathbf{q}) \cdot \nabla \phi \right| \, dT\\
  SE_y := \int_T \left| \overline{F}_3(\mathbf{q}) \cdot \nabla \phi - \widehat{F}_3(\mathbf{q}) \cdot \nabla \phi \right| \, dT
\end{eqnarray*}

These terms could be simplified a bit further (for example by factoring out $\nabla \phi$). But since the terms are rather complicated anyway, we delegated this to a computer algebra system, in our case Maple.

As we can see, computing these terms involves the terms $\phi$ that acts as a representative for \emph{one} of the basis functions $\phi_1,\dots,\phi_n$. That means we get these two error terms ($SE_x$, $SE_y$) for \emph{each} basis function. To distinguish these two, we add superscript indices to indicate the underlying basis function: $SE^i= \left(SE_x^i, SE_y^i\right)^T$ is the error term for basis function $\phi_i$.

\subsection{\texorpdfstring{Meaning of $SE$}{Meaning of SE}}
\label{sec:stiffness-analysis-what-does-se-mean}

Each component of $SE$ describes the deviation between two functions (namely the exact solution and the point-wise approximation). Due to comparing norms for our desired terms, the only thing one can say for sure is that if $SE$ is zero in either component, then the compared functions in that component are equal, i.e.\,then we have no error at all.

On the other hand, the greater $SE$ grows, the more the two compared functions deviate. However, it is unfortunately not possible to tell \emph{where} these functions deviate by just looking at $SE$.

To illustrate this, remember that $SE$ actually resembles a $L^1$ norm in two dimensions. More precisely, if we want to compute the norm of a function, we take the absolute value of it and integrate over the triangle $T$.

Consider the two functions $f_1(x,y)=(x-1)^2$ and $f_2(x,y)=(y-1)^2$. We want to compare both of them to $g(x,y)=0$. So we construct their difference $f_i(x,y)-g(x,y)$ for $i\in\{1,2\}$, and compute the norm of it:

\begin{equation*}
  \int_T \left| f_i - g \right| \, dT, \quad i \in \{1,2 \}
\end{equation*}

We can compute that
\begin{equation*}
  \int_T \left| f_1 - g \right| \, dT = \int_T \left| f_2 - g \right| \, dT = \frac{1}{4}.
\end{equation*}

Of course, $f_1 \neq f_2$, but their $L^1$norm of both functions $\frac{1}{4}$. That is we can not tell \emph{where} the functions deviate from only looking at the $L^1$ norm.

% It is clear that if $f = g$ (i.e.\,if the functions are identic), we obtain that $\left| f - g \right| = 0$, meaning that the integral gets 0, too.

% On the other hand, if we know that $\int_T \left| f-g \right|\, dT = 0$, we know that $f=g$, because -- since we take the absolute value -- we integrate only over non-negative values. If we only integrate over non-negative values and the result is 0, all values must have been 0, meaning that $f=g$.\done{Reicht das als Beispiel?}

\subsection{\texorpdfstring{How to evaluate $SE$}{How to evaluate SE}}
\label{sec:how-to-evaluate-e}

The terms $SE_x$ and $SE_y$ contain rational functions and are computationally intensive to compute exactly. It was infeasible to compute those integrals using analytical software such as Maple.

While integrating in the one-dimensional case can be approximated by the rectangle method, the two-dimensional case (i.e.\,integrating something like $\int_{0}^1 \int_0^{1-x} f(x,y)\, dy\, dx$) requires a more sophisticated approach. Instead of lines we now need to divide the area in smaller areas to sample, but at the same time make sure not to exceed the bounds. Since we are dealing with a triangle, we cannot use rectangle shapes for that.

However, it is not difficult to extend the rectangle method to the two-dimensional case. Since we are interested in an integral over a triangle $T$, and $T$ is a right triangle with leg length 1, we divide the triangle into smaller right triangles such that all triangles together cover the original triangle $T$.

Note that $\int_T f(x,y)\,dT$ can be interpreted as the volume under the function $f(x,y)$, bounded by the borders given by our triangle $T$. This, however, means that we can approximate this volume by comprising it out of smaller parts. We decided to use prisms to approximate that volume.

We chose this approximation since it is a straightforward extension of the one-dimensional midpoint rule. Even after extensive research, we were not able to find a paper that deals specifically with numerically evaluating a double integral over a triangle (there are many papers that describe double integrals over rectangles such as e.g.\,\cite{lang1987double}).

In principle, we divide the triangle in a way such that we cut the legs (of length 1) into smaller pieces of length $\frac{1}{L}$. The integer $L$ denotes the parameter for our integral approximation. Then, we can generate the centers of the resulting triangles by first computing a set of all possible centers:

\begin{equation*}
  \mathtt{POSSIBLE\_CENTERS} = \left\{
    \left(\frac{i}{L} + \frac{1}{3L}, \frac{j}{L} + \frac{1}{3L}\right) ,
    \left(\frac{i}{L} - \frac{1}{3L}, \frac{j}{L} - \frac{1}{3L}\right)
    \mid
    i, j\in \mathbb{N}
  \right\}
\end{equation*}

After this, we filter out the centers that lie outside of our triangle:

\begin{equation*}
  \mathtt{CENTERS} = \left\{
    (x,y)\in \mathtt{POSSIBLE\_CENTERS}
    \mid
    (x,y) \in T
  \right\}
\end{equation*}

If we use this definition, we obtain $L^2$ triangles of equal size.

\begin{figure}[ht!]
  \newcommand{\mytriangle}[3]{
    \begin{scope}[scale=#1*2, xshift=#2cm,yshift=#3cm]
      \draw(0,0) -- (1,0) -- (0,1) -- (0,0);
    \end{scope}
  }
  \centering
  \subfigure[One triangle($L=1$)]{
    \begin{tikzpicture}
      \mytriangle{1}{0}{0};
    \end{tikzpicture}
  }
  \subfigure[Four triangles ($L=2$)]{
    \begin{tikzpicture}
      \mytriangle{1}{0}{0};
      \mytriangle{.5}{0}{0};
      \mytriangle{.5}{0}{1};
      \mytriangle{.5}{1}{0};
    \end{tikzpicture}
  }
  \subfigure[Nine triangles ($L=3$)]{
    \begin{tikzpicture}
      \mytriangle{1}{0}{0};
      \mytriangle{.3333}{0}{0};
      \mytriangle{.3333}{0}{1};
      \mytriangle{.3333}{1}{0};
      \mytriangle{.3333}{0}{2};
      \mytriangle{.3333}{2}{0};
      \mytriangle{.3333}{1}{1};
    \end{tikzpicture}
  }
  \subfigure[Sixteen triangles ($L=4$)]{
    \begin{tikzpicture}
      \mytriangle{1}{0}{0};
      \mytriangle{.25}{0}{0};
      \mytriangle{.25}{0}{1};
      \mytriangle{.25}{1}{0};
      \mytriangle{.25}{0}{2};
      \mytriangle{.25}{2}{0};
      \mytriangle{.25}{1}{1};
      \mytriangle{.25}{0}{3};
      \mytriangle{.25}{3}{0};
      \mytriangle{.25}{1}{2};
      \mytriangle{.25}{2}{1};
    \end{tikzpicture}
  }
  \subfigure[Twenty-five triangles ($L=5$)]{
    \begin{tikzpicture}
      \mytriangle{1}{0}{0};
      \mytriangle{.2}{0}{0};
      \mytriangle{.2}{0}{1};
      \mytriangle{.2}{1}{0};
      \mytriangle{.2}{0}{2};
      \mytriangle{.2}{2}{0};
      \mytriangle{.2}{1}{1};
      \mytriangle{.2}{0}{3};
      \mytriangle{.2}{3}{0};
      \mytriangle{.2}{1}{2};
      \mytriangle{.2}{2}{1};
      \mytriangle{.2}{0}{4};
      \mytriangle{.2}{4}{0};
      \mytriangle{.2}{1}{3};
      \mytriangle{.2}{3}{1};
      \mytriangle{.2}{2}{2};
    \end{tikzpicture}
  }
  \caption{Splitting a triangle into smaller triangles, the centers of which are all contained in  $\mathtt{CENTERS}$.}
  \label{fig:splitting-triangle-into-smaller-triangles}
\end{figure}

We show how the division of the triangles works in figure \ref{fig:splitting-triangle-into-smaller-triangles}.

We can now sample the height at the centers and compute the appropriate volume of the corresponding prism. Therefore, we have to multiply the height by the base area of the respective triangle. The area of one triangle is $\frac{1}{2 L^2}$, resulting in the following equation.

\begin{equation}
  \int_T f(x,y) dT \approx \sum_{(x,y)\in\mathtt{CENTERS}} f(x,y) \cdot \frac{1}{2L^2}
\end{equation}

To clarify this, we consider some examples:

If we use just one triangle (i.e.\,$L=1$), then this triangle coincides with the original triangle, and its center is located at $(\frac{1}{3}, \frac{1}{3})$. If we use this approximation, we obtain the following:

\begin{equation*}
  \int_T f(x,y)\, dT \approx \frac{1}{2} \cdot f(\frac{1}{3},\frac{1}{3})
\end{equation*}

The factor $\frac{1}{2}$ is needed since we actually approximate the volume denoted by the integral by the volume of a prism with the same base area as our triangle $T$ (namely $\frac{1}{2}$).

Naturally we need more triangles for an actual approximation. If we use four triangles (i.e.\,$L=2$), we get the following centers: $\mathtt{CENTERS} = \left\{ (\frac{1}{6}, \frac{1}{6}), (\frac{1}{6}, \frac{1}{6}), (\frac{1}{3}, \frac{1}{6}), (\frac{1}{6},\frac{1}{3}) \right\}$.

We can now approximate the integral as follows:

\begin{equation*}
  \int_T f(x,y) dT \approx \sum_{(x,y)\in\mathtt{CENTERS}} f(x,y)\cdot \frac{1}{2\cdot 4}
\end{equation*}

Here, the factor $\frac{1}{2\cdot 4} = \frac{1}{8}$ denotes the area of one single triangle.

Using this technique, one can -- just as in the one-dimensional case -- adjust the number of triangles to take into account. The more triangles are used, the more accurate the approximation of the integral gets.

We noticed that $L=5$ (i.e.\,using 25 triangles) sufficed for high accuracy results. Higher values for $r$ very rarely resulted in visible changes to the plots and when that was the case it was never enough to make even a slight impact on the interpretation of the results.

Similar to the situation from section \ref{sec:how-to-eval-I}, we considered several options to compute $SE$. However, we experienced similar problems evaluating the integrals exactly, which is why we chose a numerical integration. We decided to implement this method ourselves, since it was a simple implementation and was easy to integrate into our parser, which works with the provided equations before it pipes the result through to Gnuplot.

\subsection{\texorpdfstring{Singularities in $SE$}{Singularities in SE}}
\label{sec:stiffness-analysis-singularities}

Before we actually generate plots and interpret them, we consider consider the term $SE_x$ for a moment (we could also do so for $SE_y$ --- it is just for illustrative purpose).

This term ($SE_x$) depends on $\widehat{F_2}(\mathbf{q})$ and $\overline{F_2}(\mathbf{q})$. Moreover, as we remember, $\widehat{F_2}(\mathbf{q})$ is defined as follows:

\begin{equation*}
  \widehat{F}_2(\mathbf{q}) \cdot \nabla \phi =
  \left( \frac{\widehat{u}_x^2}{\widehat{h}} + \frac{1}{2} g \widehat{h}^2 \right) \cdot \pd{\phi_i}{x} +
  \frac{\widehat{u}_x \cdot \widehat{u}_y }{\widehat{h}} \cdot \pd{\phi_i}{y}
\end{equation*}

As we can see, it contains the term $\widehat{h} = \sum_{i=1}^n h_i \cdot \phi_i(x,y)$ as a denominator. Since $\phi_i$ depends on $x$ and $y$, and we integrate exactly over these two variables, this might imply a problem if we have singularities (w.r.t.\,the variables $x$ and $y$), i.e.\,if $\widehat h = 0$ (which would make the denominator 0 and thus the affected fraction undefined).

Note that the term $\overline{F}(\mathbf{q})$ can not produce any singularities since $x$ and $y$ are nowhere in a denominator there (neither are they in a logarithm, or under a root).

We will now investigate when these singularities occur. We start by considering the following equation characterizing the problem cases:

\begin{equation*}
  \widehat{h} = \sum_{i=1}^n h_j \phi_j(x,y) \stackrel{!}{=} 0
\end{equation*}

If we solve this equation for e.g.\,$h_1$, we obtain
\begin{equation}
  \label{eq:stiffness-analysis-singularities-h1}
  h_1 = \frac{-\sum_{i=2}^n h_j \phi_j(x,y)}{\phi_1(x,y)}
\end{equation}

The meaning of equation (\ref{eq:stiffness-analysis-singularities-h1}) is that given the values for $h_2$ to $h_n$, the values $h_1$, $x$ and $y$ which satisfy equation (\ref{eq:stiffness-analysis-singularities-h1}) yield a singularity. We know that the point $(x,y)$ must reside within our triangle $T$ ($x+y\leq 1$ and $x\geq 0$ and $y\geq 0$). That means we can try all values $(x,y)$ that satisfy this property and compute the value $h_1$ that would produce a singularity.

We will illustrate that for some simple cases.

\begin{figure}[ht]
  \centering
  \subfigure[Heat map for first order singularities. Plot shows $h_1 = \frac{20y}{-1+2y}$. White area is either undefined or not within the triangle $T$.]{
    \label{fig:stiffness-analysis-log-singularities-ord-1}
    \includegraphics[scale=0.5]{stiffness_analysis_log_sing_o1.pdf}
  }
  \hspace{.5cm}
  \subfigure[Heat map for second order singularities. Plot shows $h_1 = \frac{20\,y^2+\left(40\,x-30\right)\,y+20\,x^2-30\,x}{2\,y^2+
        \left(4\,x-3\right)\,y+2\,x^2-3\,x+1}$. White area is either undefined or not within the triangle $T$.]{
    \label{fig:stiffness-analysis-log-singularities-ord-2}
    \includegraphics[scale=0.5]{stiffness_analysis_log_sing_o2.pdf}
  }
  \caption{Heat maps for $\log_{10} h_1$ depending on $(x,y)$ under the assumption that all other height values are set to 10. If all other height values are 10, reasonable values for $h_1$ lie in the range $\left[ 6,14 \right] \approx \left[ 10^{0.75}, 10^{1.14} \right]$. I.e. the logarithm of reasonable values for $h_1$ is in $\left[ 0.75, 1.14 \right]$. As we can see, this is never the case. This means that -- for reasonable values of $h_1$ -- we do not observe any singularities.}
\end{figure}

\subsubsection{Order 1}
\label{sec:stiffness-analysis-singluarities-ord-1}

Next, we consider order 1 and set $h_2=h_3=10$. We then obtain the following (by substituting $\phi_1$ to $\phi_3$ as defined in table \ref{tab:basis-functions-for-low-degrees}):

\begin{equation*}
  h_1 = \frac{20y}{-1+2y}
\end{equation*}

For example, we can set $x=y=0.25$ and obtain $  h_1 = \frac{1-20\cdot 0.25}{-1+2\cdot 0.25} = -8$. That means that for the point within the triangle located at $(0.25, 0.25)$ we would need a height of -8 to create a singularity. If we then take into account that $h_2=h_3=10$ we see that the value -8 strongly deviates from these values and can be considered very unlikely. Since it would be very tedious to try all values for $x$ and $y$ lying within our triangle, we can use a simple plot.

\begin{equation*}
\log_{10}\frac{20y}{-1+2y}.
\end{equation*}

We can see the graph for $h_1$ in figure \ref{fig:stiffness-analysis-log-singularities-ord-1}. Note that the scaling is logarithmic. We chose the coloring as follows:

\begin{itemize}
\item Green denotes values in $\left( -\infty, 5 \right] \cup \left[ 15, \infty \right)$. We colored these values green since they represent kind of the ``safe'' values. Remember that we are considering the case where all heights except $h_1$ are set to 10. Thus, we consider the range $\left[ 5, 15 \right]$, whose midpoint is 10.

\item Red denotes values in $\left[ 5, 15 \right]$. These represent dangerous values, since if we lie within this interval, we get a singularity even if we have non-shock waves.
\item White area is either undefined (i.e.\,it is not possible to draw on logarithmic scale, corresponding to a negative value of $h_1$) or out of the triangle.
\end{itemize}

As we can see, the value for $\frac{1-2x+24y}{-1+2y}$ is either 5 or below, or 15 or above (since there is only green in figure \ref{fig:stiffness-analysis-log-singularities-ord-1}, but no red at all). If we go back, we can conclude that either $h_1 \leq 5$ or $h_1 \geq 15$. We can conclude that -- for the values $h_2=h_3=10$ -- the only values for $h_1$ that could provoke a singularity are strongly different from 10.

The white area is undefined or out of the triangle. If the logarithm of $h_1$ is undefined, this means, that the value for $h_1$ was negative, which is also not a reasonable value for $h_1$ if all other heights are 10.

We also did experiments for other assignments of $h_2$ and $h_3$ and observed similar behavior.

\subsubsection{Order 2}
\label{sec:stiffness-analysis-singularities-ord-2}

We now examine the same thing for order 2. Again, we solve equation (\ref{eq:stiffness-analysis-singularities-h1}) for $h_1$. We do so by setting $h_2=\dots=h_6=10$.

% \begin{equation*}
%   h_1 = -\frac{\left(4\,h_{6}-2\,h_{5}\right)\,y^2+\left(\left(4\,h_{6}-4
%  \,h_{4}+4\,h_{2}\right)\,x-4\,h_{6}+h_{5}\right)\,y+\left(4\,h_{2}-2
%  \,h_{3}\right)\,x^2+\left(h_{3}-4\,h_{2}\right)\,x}{2\,y^2+\left(4\,
%  x-3\right)\,y+2\,x^2-3\,x+1}
% \end{equation*}

\begin{equation*}
  h_1 = -\frac{20\,y^2+\left(40\,x-30\right)\,y+20\,x^2-30\,x}{2\,y^2+
    \left(4\,x-3\right)\,y+2\,x^2-3\,x+1}
\end{equation*}

Again, we could test specific values for $(x,y)$, such as e.g.\,$(0.5, 0.25)$. If we did so, we would obtain $h_1=-90$ --- a very unlikely setting for $h_1$ if all other heights are 10.

Figure \ref{fig:stiffness-analysis-log-singularities-ord-2} shows a plot for $h_1$. We observe that for order 2, the value $h_1$ does not range from 5 to about 15 within our triangle. This means that $h_1$ (roughly) has to lie within the one of the ranges $\left[ -\infty, 5 \right]$ and $\left[ 15, \infty \right]$ to provoke a singularity within the triangle, which is very unlikely for small waves, but possible for a sudden increase in height (which may be a valid consideration, for example in the case of shock waves). Thus, evaluating the integral may become inaccurate under certain circumstances.

It is worthy to note at this point, that this singularity is not because of the approximation. It happens because of the sum of heights in the denominator of $\widehat{F}_2\left(\mathbf{q}\right)$ (in the term $\widehat{h}$). Our approximations do multiply the possible number of singularities because of our piece-wise integration of the triangle, however this is only of relevance in interpreting the plots if they happen to contain any of those singularities. This is not the case for us, since even the additional singularities are close to exact singularities, which are outside of the relevant height range for these calculations. That means this has no impact on our interpretation of the results, it is merely something to consider when simulating problematic scenarios.

\section{Error terms for order 1}
\label{sec:stiffness-analysis-first-touch}

We continue our analysis by investigating the case for basis functions of order 1 (for nodal DG, we have three basis functions $\phi_1,\dots,\phi_3$). Looking back at table \ref{tab:basis-functions-for-low-degrees} we derived some symmetries between the basis functions. We found out that $\phi_1(x,y)=\phi_3(y,x)$ and $\phi_2(x,y)=\phi_2(y,x)$. We will now exploit these facts.

Consider e.g.\,the term $SE_x^i = \int_T \left| \left(\overline{F}_2(\mathbf{q})-\widehat{F}_2(\mathbf{q})\right) \cdot \nabla\phi_i(x,y) \right|$. The terms $\overline{F}_2(\mathbf{q})$ and $\widehat{F}_2(\mathbf{q})$ depend on the height and momentum values of the single support points (i.e.\,on the values $h_1,\dots,h_3$, $u_{x,1},\dots,u_{x,3}$ and $u_{y,1},\dots,u_{y,3}$).

If we have two symmetric basis functions $\phi_i$ and $\phi_j$ and consider $SE^i$, we can compute the following:

\begin{equation*}
  SE^i(h, u_x, u_y) = SE^j(h, u_y, u_x)
\end{equation*}

This equation follows directly from equations \eqref{eq:point-wise-approx-result-second-line} and \eqref{eq:stiffness-analysis-second-line-exact-approx-simple}. This symmetry results in the fact that some plots look extremely similar to some others
%(in fact they might even be equal)
. This will later enable us to use some plots for multiple situations.

%This is especially useful when the $u_x$ and $u_y$-values are distributed in a way such that $u_{x,j}=u_{y,k}$ for $j=1..n$. In this case, we can completely omit the plots for the error of either the $y$- or the $x$ momentum since these plots can be constructed from their counterpart (i.e.\,the error in $x$ momentum for basis function $\phi_i$ is the same as the error in $y$ momentum for basis function $\phi_j$ if $\phi_i$ and $\phi_j$ are symmetric basis functions).

Following are reports of various scenarios we have tested, trying to find out any common behavior between the values. All the following graphs will show the computed error in the stiffness terms for both components, i.e.\,$SE^i = \left(SE_x^i, SE_y^i\right)$ for $i=1,\dots, n$.

\subsection{Order 1 --- standard values}
\label{sec:stiffness-analysis-order-1-standard}

\subsubsection{\texorpdfstring{Varying $h_1$}{Varying h1}}
\label{sec:stiffness-analysis-standard-values-var-h1}

According to section \ref{sec:basis-functions-choice}, choosing order 1 requires 3 basis functions $\phi_1,\dots,\phi_n$. These depend on the three components (height, $x$ momentum, $y$ momentum) of the three support points. First, we will look into the impact the value $h_1$ (i.e.\,the height of the first support point) has on the error terms. We keep the following values constant:

\begin{eqnarray*}
  & h_2 = 10 & h_3 = 10 \\
  u_{x,1} = 0 & u_{x,2} = 0 & u_{x,3} = 0 \\
  u_{y,1} = 0 & u_{y,2} = 0 & u_{y,3} = 0
\end{eqnarray*}

As we can see, the variable $h_1$ is explicitly omitted in the above setting, since we will leave this as the single varying component in this experiment. That means, we examine how a variation of $h_1$ results in the plots.

\input{standardwerte_nach_h1_ord1/tex.tex}

The resulting plots are displayed in figure \ref{fig:stiffness-analysis-order-1-standard-values-h1}. As we can see in these plots, the error terms $SE_x^1$ and $SE_y^3$ are 0.

The explanation is straightforward, if one considers the two identities for order 1 (see table \ref{tab:basis-functions-for-low-degrees} for reference):
\begin{eqnarray*}
  \pd{\phi_1}{x} = 0 & \pd{\phi_3}{y} = 0
\end{eqnarray*}

Now remember that $SE_x^1$ is computed by integrating over the absolute value of the following term:

\begin{align*}
  \overline{F}_2(\mathbf{q}) \cdot \nabla \phi_1 - \widehat{F}_2(\mathbf{q}) \cdot \nabla \phi_1,
\end{align*}
which can be expanded to
\begin{align*}
  \begin{pmatrix}
    \sum_{j=1}^n \left(\frac{u_{x,j}^2}{h_j^2} + \frac{1}{2} g h_j^2\right) \phi_j \\
    \sum_{j=1}^n \left(\frac{u_{x,j} u_{y,j}}{h_j}\right) \phi_j \\
  \end{pmatrix}
  \cdot
  \begin{pmatrix}
    \pd{\phi_1}{x} \\
    \pd{\phi_1}{y}
  \end{pmatrix} -
  \left( \frac{\widehat{u}_x^2}{\widehat{h}} + \frac{1}{2} g \widehat{h}^2 \right) \cdot \pd{\phi_1}{x} +
  \frac{\widehat{u}_x \cdot \widehat{u}_y }{\widehat{h}} \cdot \pd{\phi_1}{y}.
\end{align*}

Using the fact that $\pd{\phi_i}x = 0$, we can simplify the above to
\begin{align*}
    \sum_{j=1}^n \left(\frac{u_{x,j} u_{y,j}}{h_j}\right) \phi_j
    \cdot
    \pd{\phi_1}{y} -
    \frac{\widehat{u}_x \cdot \widehat{u}_y }{\widehat{h}} \cdot \pd{\phi_1}{y}.
\end{align*}

We now use the fact that $u_{x,i}=u_{y,i}=0$ for all support points and can confirm that the overall integrand becomes 0.

While this is a good result in this circumstance, the more interesting plots in figure \ref{fig:stiffness-analysis-order-1-standard-values-h1} are the remaining ones, which all seem to be the same. We can see that the error is minimal for $h_1=10$ and grows as $h_1$ deviates from this value (apparently the same in both directions). The shape is similar to a parabola, which makes sense when we examine the equations for the first order.

The fact that the error is minimal for $h_1=10$ is interesting because $h_2$ as well as $h_3$ have the value 10, too, so it might be advantageous if the third height (in this case $h_1$) has value 10, as well.

From now on, we will omit plots that depict a zero error plot and will leave a remark where necessary.

\subsubsection{\texorpdfstring{Varying $u_{x,1}$}{Varying ux1}}

We leave all values as they were and additionally set $h_1=10$, but instead take $u_{x,1}$ as the varying component. The results are depicted in figure \ref{fig:stiffness-analysis-order-1-standard-values-ux1}.

\input{standardwerte_nach_ux_ord1/tex.tex}

As we can see, we omitted all plots except those for the $x$ momentum for the second and third basis function. These two look like parabolas again, but note that the occurring error lies somewhere between 0 and 0.5.

Moreover, it is again noteworthy that the minimum error occurs for $u_{x,1}=0$ since this equals the values of $u_{x,2}$ and $u_{x,3}$.

\subsubsection{\texorpdfstring{Varying $u_{y,1}$}{Varying uy1}}

In the last section, the $x$ momentum of the second and third basis function were the most interesting terms to look at. We now focus on what happens if we choose $u_{y,1}$ as varying component.

\input{standardwerte_nach_uy_ord1/tex.tex}

The results can be seen in figure \ref{fig:stiffness-analysis-order-1-standard-values-uy1}. The plots for varying $u_{y,1}$ look like these for $u_{x,1}$, except that when varying $u_{y,1}$, the errors in the $y$ momentum of basis function 1 and 2 have the parabola-like shape. Similar to the situation before, the error is minimal for $u_{y,1}=0$.

\subsubsection{\texorpdfstring{Varying $u_{x,1}$ and $u_{y,1}$}{Varying ux1 and uy1}}

So far we restricted ourselves to one varying component, but now we will examine the situation for two variables (namely $u_{x,1}$ and $u_{y,1}$), while fixing the others to set values.

We will use the following variable assignment for the remaining variables:

\begin{eqnarray*}
  h_1 = 10 & h_2 = 10 & h_3 = 10 \\
   & u_{x,2} = 0 & u_{x,3} = 0 \\
   & u_{y,2} = 0 & u_{y,3} = 0
\end{eqnarray*}

The resulting plots are shown in figure \ref{fig:standardwerte_nach_ux1_ux2_ord1}.

\input{standardwerte_nach_ux1_uy1_ord1/tex.tex}

We focus on relevant aspects, such as that \emph{all} error plots have characteristic shapes (i.e.\,none of them is 0, like in previous examples). Moreover, it seems that the error is minimal if $u_{x,1}=u_{y,1}=0$. Once again one can see the similarities between some plots, especially with regard to mirroring/rotating. We will later omit plots that are the same or mirrored.

\clearpage{} % force output of *all* floats up to now, since LaTeX crahes otherwise

\subsection{Order 1 --- non-standard values}
\label{sec:stiffness-analysis-ord1-non-std-values}

Up until now we have only considered scenarios in which all fixed heights and momentums were equal. We will now focus on choosing different fixed values, for example $h_2=10$ and $h_3=11$.

\subsubsection{Different heights}
\label{sec:stiffness-analysis-ord1-differing-h2-10-h3-11}

We begin with a setting that employs different heights, and assigns all momentums to 0. We want to investigate the plots for varying $h_1$.

\input{ord1_differing_h2_h3_10_11/tex}

We can see the resulting plots in figure \ref{fig:stiffneses-analysis-ord1-h2-10-h3-11}. We only show the plots for the errors in $x$ momentum of second and third basis function since all other plots show 0 error again.

What is remarkable about these plots is the fact that the error is now minimal for a value of $h_1\approx 10.5$ -- which corresponds to the average value of $h_2$ and $h_3$. However, note that now the minimal error is roughly 1, while in the previous setting ($h_2=h_3=10$) the minimal error was 0.

We investigate if we can find a minimal at $h_1=11$ error if we do the same experiment for $h_2=10$ and $h_3=12$.

\input{ord1_differing_h2_h3_10_12/tex}

We can see the resulting plots in figure \ref{fig:stiffneses-analysis-ord1-h2-10-h3-12}. And, indeed, the error is now minimal at $h_1\approx 11$ -- the average of $h_2=10$ and $h_3=12$. The minimal error grew up to 4.

We will now inspect what it looks like if we set $h_2=12$ and $h_1=10$.

\input{ord1_differing_h2_h3_12_10/tex}

The resulting plots are shown in figure \ref{fig:stiffneses-analysis-ord1-h2-12-h3-10}. The minimum error is again at $h_1\approx 11$. That means, we can conjecture that it does not matter which of the other heights does differ, but only the difference between the two. This makes sense when examining the equations, which we will cover later.

In the next scenario, we set $h_2=12$ and $h_3=8$. We would expect the resulting plots to show the minimal error at $h_1=10$.

\input{ord1_differing_h2_h3_12_8/tex.tex}

As we can see in figure \ref{fig:ord1_differing_h2_h3_12_8}, the error is still minimal for a value of $h_1=\frac{12+8}{2}=\frac{h_2+h_3}{2}=10$, but the shape looks different than before. Moreover, the error terms grow up to about 19, and more importantly, the minimal error grew to over 16.

\paragraph{Conclusion}

What we can conclude from the plots seen so far is that -- if we leave all $u_{x,i}=u_{y,i}=0$ ($i=1\dots n$) and take $h_1$ as axis variable -- the error seems to be minimal if $h_1\approx \frac{h_2+h_3}{2}$.

Of course, it is an interesting question if this can be proven (or even disproven) analytically. To examine this, we take -- as an example -- the error in the $x$ momentum of the third basis function and set all $u_{x,i}=u_{y,i}=0$.

However, this does not hold true.
To illustrate this, we consider an (admittedly extreme) example where we choose $h_2=16$ and $h_3=4$.

\input{ord1_differing_h2_h3_164/tex.tex}

Figure \ref{fig:ord1_differing_h2_h3_164} shows the plot for this situation. As we can see, the error is \emph{not} minimal for $h_1=10$, even if the average of $h_2=16$ and $h_3=4$ is 10 in this case. As one would expect, the error terms become extremely large in this case (from 145 to 147 in the range from 8 to 12). Although an overall parabola form is still noticeable, there are deviations, probably due to the absolute values in the integrand which also causes the average of $h_2$ and $h_3$ to be a local maximum instead of a minimum.

\subsubsection{\texorpdfstring{Different $x$ momentums}{Different x momentums}}
\label{sec:stiffness-analysis-ord1-nonstandard-diff-ux}

For the next test, we set all height values to 10, and see what happens if we choose different values for the $u_{x,i}$. For a reference what happens for $u_{x,2}=u_{x,3}=0$, see figure \ref{fig:stiffness-analysis-order-1-standard-values-ux1}.

We start out by setting $u_{x,2}=0$ and $u_{x,3}=2$. The plots for this setting can be seen in figure \ref{fig:ord1_ux1_differing_ux2_ux3}. Again, only the $x$ momentums for the second and third basis function are shown, since all other plots show 0 error.

\input{ord1_ux1_differing_ux2_ux3/tex.tex}

We see, that the error describes a parabola-like shape, its minimum value being at $u_{x,y}\approx 1$, which -- similar to before -- resembles the average of $u_{x,2}$ and $u_{x,3}$. However, further investigating this direction is not necessary, since altering $u_{x,2}$ and $u_{x,3}$ (while leaving $u_{x,1}$ variable) affects the plots in the same way as changing $h_2$ and $h_3$ (with $h_1$ variable).

\subsubsection{\texorpdfstring{Different $y$ momentums}{Different y momentums}}
\label{sec:stiffness-analysis-ord1-nonstandard-diff-uy}

To complete the current line of experiments, we could examine different $y$ momentums analogously to before, but it is obvious from the equations that this will present the same results, only with a different zero-error functions. In that case, the $y$ momentums of the first and the second basis function are the interesting plots, while all others are 0.

\subsubsection{\texorpdfstring{Different $x$ and $y$ momentums}{Different x and y momentums}}
\label{sec:stiffness-analysis-different-ux-uy-momentums}

We now generalize the previous experiments by altering both $x$ momentums and $y$ momentums.

\input{ord1_varying_ux1_uy1_differing_ux2_uy2_1_1/tex.tex}

To visualize these settings, we will revisit three-dimensional plots that have two axis variables. Refer to figure \ref{fig:standardwerte_nach_ux1_ux2_ord1} for the original situation (i.e.\,heights are 10, momentums are 0), where we always had a minimal error at $u_{x,1}=u_{y,1}=0$. From now on, since the plots tend to be highly symmetric, we will omit some of them, stating which plots we omitted and why.

Remember that if we only changed either the $x$ or the $y$ momentum values, the error (depending on the only non-fixed $x$ or $y$ momentum) reached its minimum roughly at the average of the other two $x$ or $y$ momentums respectively, as long as they did not differ too strongly.

We start by setting $u_{x,2}=u_{y,2}=1$. The resulting plots can be seen in figure \ref{fig:ord1_varying_ux1_uy1_differing_ux2_uy2_1_1}.

Considering the plot for $SE_x^1$, it is easy to distinguish the two bent shapes (one for $u_{x,1}\approx 0.5$, the other one for $u_{y,1}\approx 0.5$ -- again roughly the average values of the corresponding $x$ and $y$ momentums). Comparing this plot to the one of figure \ref{fig:standardwerte_nach_ux1_ux2_ord1}, we can see that there the bends were sharper, while now they have been smoothed. Moreover, the error term now reaches the 0.5 mark, while previously only came up to about 0.4.

In general (i.e.\,considering all plots of figure \ref{fig:ord1_varying_ux1_uy1_differing_ux2_uy2_1_1}), we can observe a slight translation of all the plots. It is roughly 0.5 in the directions of $u_x$ and $u_y$, since we altered some $x$ and $y$ momentums. Moreover the error term grew (compared to the one where all fixed momentums were set to 0).

However, with $u_{x,2}=u_{y,2}=1$ one can still see the strong connection to the original plots.

We now choose some more extreme values: $u_{x,2}=3$ and $u_{y,2}=3$. The corresponding plots can be found in figure \ref{fig:ord1_varying_ux1_uy1_differing_ux2_uy2_3_3}.

\input{ord1_varying_ux1_uy1_differing_ux2_uy2_3_3/tex.tex}

Now, the deformations in the plots are stronger. While the overall error grew by a significant amount, we can still say that it is minimal roughly at $u_{x,1}=\frac{u_{x,2}+u_{x,3}}{2}$ and $u_{y,1}=\frac{u_{y,2}+u_{y,3}}{2}$.

What we can assume from the plots we have seen so far is that the more the momentums differ, the worse our approximation (i.e.\,the bigger the error) will get. We made the same observation about the height values before.

To examine this suspicion a bit, we now choose $u_{x,1}=u_{x,2}=u_{y,2}=u_{y,3}=1$ assuming that this should roughly look like the original situation where all moments were set to 0, except that we suspect the error to be minimal at $u_{x,1}=1$ and $u_{y,1}=1$.

\input{ord1_varying_ux1_uy1_differing_ux2_uy2_ux3_uy3_1_1_1_1/tex.tex}

The plots confirm our suspicion: the error is indeed minimal for our suspected values of $u_{x,1}$ and $u_{y,1}$ -- it is even 0.

\clearpage{}

\subsection{Order 1 --- Different orders of magnitude}
\label{sec:ord1-different-orders-magnitude}

So far we have seen that the more the heights and momentums differ the larger the error grows. We will now examine whether the \emph{difference} in heights and momentums is the only thing that affects the error, i.e. if the difference of stays the same, but the order of magnitude varies.

\subsubsection{Standard values}
\label{sec:ord1-diff-ord-magnitude-std-values}

First we consider two exemplary cases using default values in two different orders of magnitude.

\begin{itemize}
\item In the first case, we leave $h_2=h_3=10$ and $u_{x,1}=u_{x,2}=u_{x,3}=u_{y,1}=u_{y,2}=u_{y,3}=0$. The variable $h_1$ is the non-fixed variable. The range for $h_1$ will be $[10-2, 10+2]=[8,12]$.
\item In the second case, we leave $h_2=h_3=1000$ and $u_{x,1}=u_{x,2}=u_{x,3}=u_{y,1}=u_{y,2}=u_{y,3}=0$. The variable $h_1$ is the non-fixed variable. The range for $h_1$ will be $[1000-2, 1000+2]=[998,1002]$.
\end{itemize}

The difference in heights and momentums is the same for both cases, (4 units for $h_1$ and 0 units for all other variables). However, the total differs by a factor of $10^2$. We will restrict ourselves to errors for certain basis functions, as the effects experienced there are similar for other basis functions.

\input{magnitude_comparison_default/tex}

Figure \ref{fig:magnitude_comparison_default} compares the two scenarios described above (for the $x$ momentum of the 2nd basis function --- similar effects can be seen for all basis functions and components). We see two plots that look identical. This could lead to the conclusion that only the difference of the heights and momentums affects the error instead of the absolute values. However, that is not the case as we will see below.

\subsubsection{Differing height values}
\label{sec:ord1-magnitude-non-std-values1}

We will now slightly alter the above scenario to the following:

\begin{itemize}
\item In the first case, we leave $h_2=12, h_3=7$ and $u_{x,1}=u_{x,2}=u_{x,3}=u_{y,1}=u_{y,2}=u_{y,3}=0$. The variable $h_1$ is the non-fixed variable. The range for $h_1$ will be $[10-2, 10+2]=[8,12]$.
\item In the second case, we leave $h_2=1002, h_3=997$ and $u_{x,1}=u_{x,2}=u_{x,3}=u_{y,1}=u_{y,2}=u_{y,3}=0$. The variable $h_1$ is the non-fixed variable. The range for $h_1$ will be $[1000-2, 1000+2]=[998,1002]$.
\end{itemize}

We ensured again that the \emph{difference} in heights and momentums is the same for both cases. We show two resulting plots for the $x$ momentum of the second basis function in figure \ref{fig:magnitude_comparison_differing_heights}. Similar behaviour is seen for all other basis functions aswell.

\input{magnitude_comp_nonstd1/tex}

Again, these plots look the same. We conducted further experiments only adjusting the height values accordingly and experienced same plots for different orders of magnitude.

\subsubsection{Differing momentums}
\label{sec:ord1-magnitude-differing-momentums}

The next scenario to examine is varying momentum values. We will use the same values as above, only with non-zero momentums.

Therefore, we consider the following two scenarios:

\begin{itemize}
\item In the first case, we set $h_2=h_3=10$ and $u_{x,1}=2, u_{x,2}= -2, u_{x,3}= 2, u_{y,1}= -2, u_{y,2}= 4, u_{y,3}=1$. The variable $h_1$ is the non-fixed variable. The range for $h_1$ will be $[8, 12]$.
\item In the second case, we set $h_2=h_3=1000$ and $u_{x,1}=2, u_{x,2}= -2, u_{x,3}= 2, u_{y,1}= -2, u_{y,2}= 4, u_{y,3}=1$. The variable $h_1$ is the non-fixed variable. The range for $h_1$ will be $[998,1002]$.
\end{itemize}

\input{magnitude_comp_momentums/tex}

The resulting plots can be seen in figure \ref{fig:magnitude_comp_momentums}. Contrary to before, we see different behavior for the two plots. Surprisingly, we see that the error can be zero for the second case (where the height values are around 1000), and does not reach zero in the first case (height values around 10).

\subsubsection{Differing heights and momentums}
\label{sec:ord1-magnitude-differing-heights-momentums}

We will now research a combination of differing heights and momentums. The two cases we consider are as follows:

\begin{itemize}
\item In the first case, we set $h_2=12, h_3=9$. The variable $h_1$ is the non-fixed variable. The range for $h_1$ will be $[8, 12]$.
\item In the second case, we set $h_2=1002, h_3=999$ . The variable $h_1$ is the non-fixed variable. The range for $h_1$ will be $[998,1002]$.
\end{itemize}

For both cases, we use $u_{x,1}=2, u_{x,2}= -2, u_{x,3}= 2, u_{y,1}= -2, u_{y,2}= 4, u_{y,3}=1$.

\input{magnitude_comp_heights_momentums/tex}

Figure \ref{fig:magnitude_comp_heights_momentums} shows the error term of the $x$ momentum for the first basis function and the $x$ momentum for the second basis function. Surprisingly, these two scenarios yield different plots for the $x$ momentum of the first basis function. The error in the $x$ momentum for second basis function differ. \todo{Macht der Satz Sinn?}

That shows that not only the difference in height values play a role for the total error, but also the momentums. However, we still see that the errors do not differ too strongly for the different orders of magnitude, meaning the actual error values can be approximated with results obtained from test cases with low water depth.

\clearpage{}

\section{Error terms for order 2}
\label{sec:stiffness-analyis-ord-2}

We will now examine the error plots for order 2. Remember from table \ref{tab:basis-functions-for-low-degrees} that we now have 6 basis functions $\phi_1,\dots,\phi_6$. Besides making the approximation more complicated, this gives rise to 12 plots (we have the errors in $x$ momentum and in $y$ momentum for each of the basis functions).

Similar to before, we will use only one plot for several error plots if they all look the same.

\subsection{Order 2 --- Standard values}
\label{sec:stiffness-analyiss-ord2-default}

\subsubsection{\texorpdfstring{Varying $h_1$}{Varying h1}}
\label{sec:stiffness-analysis-ord2-default-var-h1}

Again, we start out by using the default values, setting all heights to 10 (except $h_1$ which will be our axis variable) and all momentums will be set to 0. We can see the resulting plots in figure \ref{fig:ord2_varying_h1_default}.

\input{ord2_varying_h1_default/tex.tex}

As one can see, we always have a parabola-like shape (except for the $x$ momentum of the 5th and the $y$ momentum of the 3rd basis function that are both 0). The error is at most at 1.6 (for $SE_x^2$). We observe that the error is always minimal (in fact even 0) for $h_1=10$. The results of $SE_x^5$ and $SE_y^3$ are a natural consequence of $\pd{\phi_5}{x}=\pd{\phi_3}{y}=0$, analogous to the order 1 case.

\subsubsection{\texorpdfstring{Varying $u_{x,1}$}{Varying ux1}}
\label{sec:stiffness-analysis-ord2-default-var-ux1}

Next we choose $u_{x,1}$ to be the axis variable. We set all other momentums to 0 and all height values to 10. If we do so, we obtain the plots shown in figure \ref{fig:ord2_varying_ux_1}.

\input{ord2_varying_ux_1/tex.tex}

We can see there that the error for all $y$ momentums is zero. Moreover, the error is zero for the $x$ momentum of the 5th basis function. The error becomes zero at $u_{x,1}=0$, i.e.\,if $u_{x,1}$ is the same as all other momentums.

For the most part, the plots all look parabola-like which is traceable since the error term contains $\widehat{u}_x^2$, which itself contains $u_{x,1}^2$. Thus, we can explain the parabola-like shape.

The fact that all $y$ momentum errors are zero deserves a special attention. However, we will explain this fact in a more generalized setting --- we will not restrict ourselves to second order and will assume that all heights are equal (not exactly 10).

We start out by examining the error of the $y$ momentum for the basis function $\phi_X$. To do so, we define $\mathtt{DIFFY}:=\left( \overline{F}_3(\mathbf{q}) - \widehat{F}_3(\mathbf{q}) \right)$. That makes $\mathtt{DIFFY}$ a two-component vector containing an $x$ and an $y$ coordinate.

\begin{align*}
  SE_y^X &= \int_T \left| \left( \overline{F}_3(\mathbf{q}) - \widehat{F}_3(\mathbf{q}) \right) \cdot \nabla \phi_X \right| \, dT = \\
  &= \int_T \left| \mathtt{DIFFY} \cdot \nabla \phi_X \right|\,dT = \\
  &= \int_T \left| \left( \mathtt{DIFFY}_X \cdot \pd{\phi_X}{x} + \underbrace{\mathtt{DIFFY}_Y}_{=0} \cdot \pd{\phi_X}{y} \right) \right| \, dT = \\
  &=\int_T \left| \left( \mathtt{DIFFY}_X \cdot \pd{\phi_X}{x} \right) \right| \, dT
\end{align*}

We could simplify $\mathtt{DIFFY}_Y$ to 0 because $\mathtt{DIFFY}_Y=\sum_{j=1}^n\left( \frac{u_{x,j}\cdot u_{y,j}}{h_j} \right)-\frac{\widehat{u_x}\cdot\widehat{u_y}}{\widehat{h}}$. If all $y$ momentums are 0 (which is the case in our scenario), then this term becomes 0.

Consider the term $\mathtt{DIFFY}_X$ that describes the difference in the $x$ component between the exact solution and the approximation:

\begin{equation*}
  \mathtt{DIFFY}_X = \left( \sum_{j=1}^n \left(\frac{u_{x,j}^2}{h_j} + \frac{1}{2} g h_j^2\right) \phi_j \right) - \left( \frac{\widehat{u}_x^2}{\widehat h} + \frac{1}{2} g \widehat{h}^2\right)
\end{equation*}
with $\widehat h, \widehat{u}_x, \widehat{u}_y$ as defined in equation (\ref{eq:substitutions-for-all-components-with-hat}).

If we choose all momentums to be 0 (which is what we are doing in this scenario), then we can simplify $\mathtt{DIFFY}_X$ to the following:

\begin{equation*}
  \left( \sum_{j=1}^n \left(\frac{1}{2} g h_j^2\right) \phi_j \right) - \left( \frac{1}{2} g \widehat{h}^2\right)
\end{equation*}

We have $\sum_{i=1}^n \phi(x,y)=1$ and  $(\sum_{i=1}^n a \phi(x,y))^2 = a^2$ as computed in section \ref{sec:basis-functions-other-properties}.

With these two identities and $\widehat{h}=\sum_{j=1}^n \left(h_j \phi_j\right)$, we can set $h_1=h_2=h_3=\dots=h_n$ (remember that we fixed all height values at 10) and simplify as follows:

\begin{align*}
  \left( \sum_{j=1}^n \left(\frac{1}{2} g h_j^2\right) \phi_j \right) - \left( \frac{1}{2} g \widehat{h}^2\right) &= \left( \sum_{j=1}^n \left(\frac{1}{2} g h_j^2\right) \phi_j \right) - \left( \frac{1}{2} g \left( \sum_{j=1}^n h_j\phi_j \right)^2\right) = \\
  &= \left( \sum_{j=1}^n \left(\frac{1}{2} g h_1^2\right) \phi_j \right) - \left( \frac{1}{2} g \left( \sum_{j=1}^n h_1\phi_j \right)^2\right) = \\
  &= \frac{1}{2} g h_1^2 \cdot \sum_{j=1}^n \phi_j - \frac{1}{2} g h_1^2 = \\
  &= 0
\end{align*}

That means, we can simplify the above to 0, thereby explaining the result. If all heights are equal, and all $x$ momentums are 0, then the error for all $y$ momentums will be 0.

\subsection{Order 2 --- Non-standard values}
\label{sec:stiffness-analysis-ord2-nondefault}

Similar to order 1, we will now alter some values and see how this affects our error plots. We will do similar alterations to the ones used in section \ref{sec:stiffness-analysis-ord1-non-std-values} and see if we have a similar behavior to before.

\subsubsection{Altering heights}
\label{sec:stiffness-analysis-ord2-nondefault-altering-heights}

\paragraph{\texorpdfstring{Altering $h_2$}{Altering h2}}

We choose $h_2=11$ and leave all other height values ($h_3$ to $h_6$) fixed at 10. The resulting plots can be seen in figure \ref{fig:ord2_varying_h1_h2_11}.

\input{ord2_varying_h1_h2_11/tex.tex}

As we can see, we have a strongly different situation here compared to that for order 1, where we set one single height to 11 (section \ref{sec:stiffness-analysis-ord1-differing-h2-10-h3-11}). All of the plots assume the shape of a fourth degree polynomials.

This is traceable, since we use second order approximations for $h, u_x$ and $u_y$ (due to the second order basis functions) that are multiplied with each other. This results in the fourth degree.

While in the first order case, we saw that the error was minimal roughly at the average values of the fixed heights (at least if we altered only one height value to 11), this does not hold true anymore. Trying to derive the local minima analytically is made too difficult because of the absolute value used in the integrand.

As expected from our computations in section \ref{sec:stiffness-analysis-ord2-altering-momentums} and \ref{sec:basis-functions-other-properties}, the errors for the $y$ momentum of the 3rd basis function and the $x$ momentum of the 5th basis function is still 0. For a detailed computation, refer to section \ref{sec:stiffness-analysis-ord2-altering-momentums}.

\paragraph{\texorpdfstring{Altering $h_3$}{Altering h3}}

We now want to get insights whether modifying another height value results in remarkably different plots. Therefore, we choose $h_3=11$ and consider the resulting plots in figure \ref{fig:ord2_varying_h1_h3_11}.

\input{ord2_varying_h1_h3_11/tex.tex}

First of all, we notice that $SE_y^3$ and $SE_x^5$ remain zero, which is not surprising considering the above.

However, all other plots still resemble polynomials of the fourth degree. Again, we can not be sure that the minimum error is at the average of the height values $h_2$ to $h_6$. Moreover, all these errors are \emph{never} 0.

What we can see from these plots is that it has a significant influence which of the height values we modify. Even if we still look at 4th degree polynomials, they have significantly different shapes than the ones obtained by setting $h_2=11$.

\paragraph{Altering all height values}

We now set all heights $h_2$ to $h_6$ to the value 11. We do so, because we want to examine if we now again obtain parabola-like shapes (as in the case where all heights were set to 10). The results are shown in figure \ref{fig:ord2_varying_h1_h2-h6_11} and confirm our expectations, parabola-like shapes with minima at $h_1=11$ (with no exceptions).

\input{ord2_varying_h1_h2-h6_11/tex.tex}

Even the opening of the parabola looks very similar, and the minimum error is 0.

\paragraph{Differing heights}

Up to now we examined what happens if all heights are equal or at most one height differs from the other ones. We now do an experiment where we set $h_2=h_3=11$ and $h_4=h_5=h_6=9$. The corresponding plots are shown in figure \ref{fig:ord2_varying_h1_h2-h3_11_h4-h6_9}.

\input{ord2_varying_h1_h2-h3_11_h4-h6_9/tex.tex}

Now we have the first time where some plots look strongly different than before.

\clearpage{}

\subsubsection{Altering momentums}
\label{sec:stiffness-analysis-ord2-altering-momentums}

\paragraph{\texorpdfstring{Altering $u_{x,2}$}{Altering ux2}}

We will now examine what influence has it if we alter one single $x$ momentum. Therefore, we choose $u_{x,2}=1$ and leave all other momentums at 0. We show the corresponding plots in figure \ref{fig:ord2_varying_ux2_1}.

\input{ord2_varying_ux2_1/tex.tex}

We can see that the curves are no parabolae anymore, but look more like functions of degree 4. Again, this is convincing, since we are multiplying approximations of degree 2 with each other, which results in degree 4. However, the errors in the $x$ momentum of the 5th basis function and the $y$ momentums of all basis functions are still 0.

\paragraph{\texorpdfstring{Altering $u_{x,3}$}{Altering ux3}}

We now want to research whether the different $x$ momentums have significantly different impacts on the error term. Therefore, we now exemplarily alter $u_{x,3}$ (instead of altering $u_{x,2}$, like before). The plots are shown in figure \ref{fig:ord2_varying_ux1_ux3_1}.

\input{ord2_varying_ux1_ux3_1/tex.tex}

The plots look quite similar to the ones where we changed $u_{x,2}$, even if the impact seems to be a bit weaker at some points (especially at the $x$ momentums of the 1st, 2nd and 3rd basis function). Therefore, we can suspect that the error plots will look quite similar if we only alter one single $x$ momentum, i.e.\,it does not (significantly) matter which $x$ momentum we alter, but only how strong we do so.

\paragraph{\texorpdfstring{Altering $u_{y,2}$}{Altering uy2}}

We will now not change the $x$ momentum but the $y$ momentum. As a showcase example, we will use $u_{y,2}=1$ and leave all other momentums at 0 (-- all heights are still 10).

The plots are shown in figure \ref{fig:ord2_varying_ux1_uy2_1} and are likely to show important features. Because these plots are quite insightful, we split them up into the errors for the $x$ momentums and the errors for the $y$ momentums.

The errors for the $x$ momentums are shown in figure \ref{fig:ord2_varying_ux1_uy2_1}, the ones for the $y$ momentums in figure \ref{fig:ord2_varying_ux1_uy2_1__ycomponent}.

\subparagraph{\texorpdfstring{Errors in $x$ momentums}{Errors in x momentums}}

We first concentrate on the errors of the $x$ momentums shown in figure \ref{fig:ord2_varying_ux1_uy2_1}.

\input{ord2_varying_ux1_uy2_1/tex.tex}

All in all, the error plots look roughly the same, except for the error of the 5th basis function. All of the other plots still show parabolae, and the error is negligibly bigger than the error used for default values (use figure \ref{fig:ord2_varying_ux_1} for comparison). The most striking difference is the $x$ momentum for the 5th basis function, that has changed from zero to an absolute-value linear function.

Note that all errors (for $x$ momentum) reach zero for $u_{x,1} = 0$. We can confirm this result analytically. Again, we choose a slightly generalized setting as follows:

\begin{itemize}
\item All heights values are equal.
\item All $x$ momentums are 0.
\item The $y$ momentums can be chosen ad libitum.
\end{itemize}

Note that the plots show a special case of these three conditions.

We start analyzing by considering (once again) $SE_x^i$:

\begin{align*}
  SE_x^i = \int_T \left| \left(\overline{F_2}(\mathbf{q}) - \widehat{F_2}(\mathbf{q})\right) \cdot \nabla \phi_i \right|\, dT
\end{align*}

We will focus on the integrand and show that it becomes 0 for $u_{x,1}=0$. We continue by examining $\overline{F_2}(\mathbf{q})$ for the desired setting (all height values the same, all $x$ moments are 0, $y$ moments arbitrary):

\begin{equation}
  \label{eq:1}
  \overline{F_2}(\mathbf{q}) =
  \begin{pmatrix}
    \sum_{j=1}^n \left( \frac{u_{x,j}^2}{h_j} + \frac{1}{2} g h_j^2 \right) \cdot \phi_j \\
    \sum_{j=1}^n \left( \frac{u_{x,j}\cdot u_{y,j}}{h_j} \right) \cdot \phi_j
  \end{pmatrix} =
  \begin{pmatrix}
    \sum_{j=1}^n \frac{1}{2} g h_j^2 \phi_j \\
    0
  \end{pmatrix} =
  \begin{pmatrix}
    h_1^2 \\ 0
  \end{pmatrix}
\end{equation}

Analogously, we proceed for $\widehat{F_2}(\mathbf{q})$:

\begin{equation}
  \label{eq:2}
  \widehat{F_2}(\mathbf{q}) =
  \begin{pmatrix}
    \frac{\widehat{u}_x^2}{\widehat h} + \frac{1}{2} g \widehat h ^2 \\
    \frac{\widehat u_x \cdot \widehat u_y}{\widehat h}
  \end{pmatrix} =
  \begin{pmatrix}
    \frac{1}{2} g \left( \sum_{j=1}^n h_j \phi_j \right) ^2 \\
    0
  \end{pmatrix} =
  \begin{pmatrix}
    h_1^2 \\ 0
  \end{pmatrix}
\end{equation}

From equations (\ref{eq:1}) and (\ref{eq:2}) we can see the reason that we can choose the $y$ momentums as we like it --- it will not influence the error term $SE_x^i$.

This means, we can simplify as follows (for the current scenario):

\begin{equation*}
  SE_x^i = \int_T \left| \left(\overline{F_2}(\mathbf{q}) - \widehat{F_2}(\mathbf{q})\right) \cdot \nabla \phi_i \right|\, dT = \int_T \left|
    \left(
      \begin{pmatrix}
        \frac{1}{2} g h_1^2 \\ 0
      \end{pmatrix} -
      \begin{pmatrix}
        \frac{1}{2} g h_1^2 \\ 0
      \end{pmatrix}
    \right)
    \cdot \nabla \phi_i \right|\, dT =
  0
\end{equation*}

\subparagraph{\texorpdfstring{Errors in $y$ momentums}{Errors in y momentums}}

The error plots for the $y$ momentums are shown in figure \ref{fig:ord2_varying_ux1_uy2_1__ycomponent} and does not look as structured anymore (compared to before where they were all 0).

\input{ord2_varying_ux1_uy2_1/tex_y.tex}

Now the errors in the $y$ momentums are not zero anymore, but look like piecewise linear functions (at least within the plotted range). Even if the error term did not grow that much, the shape differs strongly from what we have seen so far.

Probably most interesting is $SE_y^5$, which is now constant and almost zero. This can be explained by $\pd{\phi_5}{x}=0$, which leads to the following derivation:

\begin{eqnarray*}
  SE_y^5 & := & \int_T \left| \left(\overline{F}_3\left(\mathbf{q}\right) - \widehat{F}_3\left(\mathbf{q}\right)\right) \cdot \begin{pmatrix}\pd{\phi_5}{x} \\ \pd{\phi_5}{y}\end{pmatrix}\right| \\
  {} & = & \int_T \left| \left(\overline{F}_3\left(\mathbf{q}\right) - \widehat{F}_3\left(\mathbf{q}\right)\right) \cdot \begin{pmatrix}0 \\ \pd{\phi_5}{y}\end{pmatrix}\right| \\
  {} & = & \int_T \left| \left( \sum_{j=1}^n \left(\frac{u_{y,j}^2}{h_j} + \frac{1}{2} g h_j^2\right) \phi_j - \left( \frac{\widehat{u}_y^2}{\widehat{h}} + \frac{1}{2} g \widehat{h}^2 \right) \phi_j \right) \cdot \left(\pd{\phi_5}{y}\right) \right| \\
\end{eqnarray*}

We already see here that $u_{x,1}$ is not present, and since all other variables are set to specific values, that implies that the resulting value must be a constant. Seeing how all $h_j$ are 10, we can treat them as a constant $h$ and factor them out of both sums, where they eliminate each other, simplifying the term to this:

\begin{eqnarray*}
  SE_y^5 & = & \int_T \left| \left( \sum_{j=1}^n \frac{u_{y,j}^2}{h} \phi_j - \frac{\widehat{u}_y^2}{\widehat{h}} \right) \cdot \left(\pd{\phi_5}{y}\right) \right|\, dT \\
  {} & = & \int_T \left| \left( \sum_{j=1}^n \frac{u_{y,j}^2}{h} \phi_j - \frac{\left(\sum_{j=1}^n u_{y,j}\phi_j\right)^2}{\sum_{j=1}^n h\phi_j} \right) \cdot \left(\pd{\phi_5}{y}\right) \right|\, dT \\
\end{eqnarray*}

Using that $\sum_{j=1}^n \phi_j = 1$ and that all $u_{y,j} = 0$ for $j \neq 2$, we finally obtain:

\begin{eqnarray*}
  SE_y^5 & = & \int_T \left| \left( \sum_{j=1}^n \frac{u_{y,j}^2}{h} \phi_j - \frac{\left(\sum_{j=1}^n u_{y,j}\phi_j\right)^2}{\sum_{j=1}^n h\phi_j} \right) \cdot \left(\pd{\phi_5}{y}\right) \right|\, dT \\
  {} & = & \int_T \left| \left( \frac{1}{h} \sum_{j=1}^n u_{y,j}^2 \phi_j - \frac{1}{h} \cdot \frac{\left(\sum_{j=1}^n u_{y,j}\phi_j\right)^2}{\sum_{j=1}^n \phi_j} \right) \cdot \left(\pd{\phi_5}{y}\right) \right|\, dT \\
{} & = & \int_T \left| \left( \frac{1}{h} u_{y,2}^2 \phi_2 - \frac{1}{h} \left(u_{y,2}\phi_2\right)^2 \right) \cdot \left(\pd{\phi_5}{y}\right) \right|\, dT \\
	{} & = & \int_T \left| \left( \frac{u_{y,2}^2 \phi_2}{h} \left( 1 - \phi_2\right) \right) \cdot \left(\pd{\phi_5}{y}\right) \right|\, dT
\end{eqnarray*}

We saw that changing an $y$ momentum can result in drastically changed shapes of the error plots,  which is something we did not expect. We examined several other assignments for $u_{y,i}$ while leaving the heights and $x$ momentums as they were in this experiment and observed similar piecewise linear function plots for $SE_y^i$.

\paragraph{Setting all $x$ momentums to 1}

We will now come back to only modifying the $x$ momentums. We will set them all to 1 and leave $u_{x,1}$ as axis variable. We obtain parabolae with minima at $u_{x,1}=1$ (since all other $x$ momentums are 1) that are shown in figure \ref{fig:ord2_varying_ux1_ux2-ux6_1}.

\input{ord2_varying_ux1_ux2-ux6_1/tex.tex}

\subsection{Order 2 --- Different orders of magnitude}
\label{sec:ord2-different-orders-magnitude}

As in the previous section, we will now analyze the behavior concerning different orders of magnitude and its effects on the error term.

\subsubsection{Standard values}
\label{sec:ord2-diff-ord-magnitude-std-values}

Again we consider two exemplary cases using default values in two different orders of magnitude.

\begin{itemize}
\item In the first case, we leave $h_i=10$ for $i \in \{2,3,4,5,6\}$ and $u_{x,i}=u_{y,i}=0$ for $i \in \{1,2,3,4,5,6\}$. The variable $h_1$ is the non-fixed variable. The range for $h_1$ will be $[10-2, 10+2]=[8,12]$.
\item In the second case, we leave $h_i=1000$ for $i \in \{2,3,4,5,6\}$ and $u_{x,i}=u_{y,i}=0$ for $i \in \{1,2,3,4,5,6\}$. The variable $h_1$ is the non-fixed variable. The range for $h_1$ will be $[1000-2, 1000+2]=[998,1002]$.
\end{itemize}

As before, the difference in heights and momentums within one scenario is the same for both cases, (4 units for $h_1$ and 0 units for all other variables) while the total differs by a factor of $10^2$. Once more we experience that the effects between basis functions were very similar, which is why we will only show selected representative plots.

\input{ord2_magnitude_comparison_default/tex}

Figure \ref{fig:ord2_magnitude_comparison_default} compares the two scenarios described above. There is no difference between the two, meaning that with one sole differing height value the error does not depend on the total magnitude.

\subsubsection{Differing height values}
\label{sec:ord2-magnitude-non-std-values1}

Now we consider variable height values, but will leave the momentums zeroed still:

\begin{itemize}
\item In the first case, we leave $h_2=12, h_3=7, h_4=15, h_5=11, h_6=9$ and $u_{x,i}=u_{y,i}=0$ for $i \in \{1,2,3,4,5,6\}$. The variable $h_1$ is the non-fixed variable. The range for $h_1$ will be $[10-2, 10+2]=[8,12]$.
\item In the second case, we leave $h_2=1002, h_3=997, h_4=1015, h_5=1011, h_6=999$ and $u_{x,i}=u_{y,i}=0$ for $i \in \{1,2,3,4,5,6\}$. The variable $h_1$ is the non-fixed variable. The range for $h_1$ will be $[1000-2, 1000+2]=[998,1002]$.
\end{itemize}

\input{ord2_magnitude_comparison_nonstd1/tex}

Figure \ref{fig:ord2_magnitude_comparison_momentums} compares the two scenarios. Again we cannot discern a difference between the two, which leads to the conclusion that with only differing height the error does not increase with magnitude.

\subsubsection{Differing heights and momentums}
\label{sec:ord2-magnitude-differing-heights-momentums}

We now do research a combination of differing heights and momentums. The two cases we consider are as follows:

\begin{itemize}
	\item In the first case, we leave $h_2=12, h_3=7, h_4=15, h_5=11, h_6=9$ and $u_{x,i}=(-1)^i \cdot i, u_{y,i}=(-1)^{i+1} \cdot i$ for $i \in \{1,2,3,4,5,6\}$. The variable $h_1$ is the non-fixed variable. The range for $h_1$ will be $[10-2, 10+2]=[8,12]$.
\item In the second case, we leave $h_2=1002, h_3=7, h_4=1015, h_5=1011, h_6=999$ and $u_{x,i}=(-1)^i \cdot i, u_{y,i}=(-1)^{i+1} \cdot i$ for $i \in \{1,2,3,4,5,6\}$. The variable $h_1$ is the non-fixed variable. The range for $h_1$ will be $[1000-2, 1000+2]=[998,1002]$.
\end{itemize}

\input{ord2_magnitude_comparison_heights_momentums/tex}

Figure \ref{fig:ord2_magnitude_comparison_heights_momentums} shows two cases, the $x$ and $y$ momentums of basis function 5.
We can see that the $y$ momentum plots look almost the same, with only slightly different scaling, similar to our previous results from section \ref{sec:ord1-magnitude-differing-heights-momentums}.
The $x$ momentum is interesting because it actually exhibits a different shape.
That can be explained by looking at the basis function 5, because differentiated by $x$ it is zero.
Since that value is multiplied onto a part of the equation (the $x$ part of $\overline{F}_2\left(\mathbf{q}\right)$ and $\widehat{F}_2\left(\mathbf{q}\right)$), parts of the error term are zeroed entirely which results in an overall small error.
This also means that the error difference between the terms of different magnitudes are emphasized, which is why the plots look different.

Aside from this case (and the $y$ momentum of basis function 3, for the same reason) the plots look very similar in shape and only differ in their scaling.
Occasionally the error is even less with higher overall water depth, implying that the error introduced through magnitude isn't a monotone function in height, but depends on other variables and their distribution.

\subsubsection{Conclusion}
\label{sec:ord2-magnitude-differing-conclusion}

From this we can conclude that in cases with varying momemtums (which are all realistic cases) the resulting error in the calculations will vary with height magnitude.
However, not only does it not differ strongly, it is often also lower.
This means that it's not a monotone error and will have very little practical relevance, as the it is significantly lower than the error introduced from other sources to not affect the end result.

The magnitude error also depends on the actual values and setup of basis functions to determine the exact relation between different heights, making it inpractical to determine for the general case.
If that value is of interest, it will have to be examined within a certain scenario using known basis functions.

\remark{Vielleicht mathematisch zeigen, dass Null-Impuls nichts am Term aendert?}

\clearpage{}

\section{Conclusion}
\label{sec:stiffness-analysis-conclusion}

\subsection{Minimizing the error}
\label{sec:stiffness-analysis-concl-error-0}

Of the three components, only the second and third ($u_x$ and $u_y$) were relevant to us, as the height error itself is always zero for this approximation, as explained in section \ref{sec:stiffness-height-error}.
Furthermore, there was strong symmetry observed for error behavior in the $u_x$ and $u_y$ components.
This means that the error can be assumed the same for both components, although the situations may be mirrored depending on the order of basis functions, i.e.\,the error in $u_x$ component will also appear in the $u_y$ component, but possibly for a different basis function.

If we have a situation where all heights are the same, all $x$ momentums are the same and all $y$ momentums are the same, the error becomes 0 for both components.
For the respective computations, refer to the previous sections.

The error can become zero for other combinations as well, but deriving a pattern is a non-trivial task and not generally useful, as it relies on a large number of parameters.
Moreover it can become zero if certain basis functions have a zero derivative (as seen e.g.\,in section \ref{sec:stiffness-analysis-standard-values-var-h1}).

The general pattern, which is observable in every plot, is that the error is proportional to the overall deviation of the various components.
The lower the deviation of the heights, $x$ momentums and $y$ momentums is the lower the resulting error, except for a few detailed plots which differ in localized areas, but still exhibit the same overall shape.
That speaks in favor of the approximation, because the values generally tend to be close together, even more so the smaller the chosen triangles are.
However, this is still a point to consider when the simulation can contain extreme scenarios, such as shock waves, or if the chosen triangle size is not small enough due to concerns for efficiency.

\subsection{Higher orders}
\label{sec:stiffness-analysis-conclusion-higher-order}

We also examined the situation for higher orders. While the plots look similar in shape to the plots we have seen so far, the error value is larger. Conversely, for order 1 the error disappears, which makes sense, as the approximation we used can be shown to be exact for order 0.

While the error increases with order, the explanations for these plots are analogous to the ones we have given for order 1 and order 2, even if the computations are more expensive and lengthy. For that same reason, we did not exhaustively test higher orders, but enough to confirm the relation with orders 1 and 2 in the shape of the error plots.

\subsection{Assessment}

It is difficult to make accurate claims of how well the approximation holds up in practice based on the data gathered here, because the $L^1$ norm absorbs some information that might be useful and what we end up with is just a norm, not an accurate measure.

However, getting such a measure is impossible without actually trying it on a large-scale data set with a full implementation and a powerful architecture to compute the non-approximated version, both of which go beyond the scope of this project. It can still be used to make qualitative observations on the data, which suggest that the used approximations hold up for values within a certain range of each other, which, as stated above, is usually the case. The only exceptions to that are radical sample occurrences like shock waves, which have to be handled with specific care because of this. For example, on an adaptive grid, this could be done by further dividing the triangles further at that point, to reduce the height differences within each of them.

Moreover, our experiments have shown that there are situations in which the approximate solution is equal to the exact solution. Moreover we saw that it is quite hard to predict how
the curves will behave without actually plotting them.

The practical merits of this approximation are already established, as it has been used in various software packages up to this point. However, an empirical study on the exact effects of the approximation would be useful in determining the precise accuracy, which could be a deciding factor on which method to use in a simulation framework, especially considering factors that go beyond the theory presented here and apply to a live implementation.

\clearpage

\bibliographystyle{alpha}
\bibliography{bibliography}

\appendix

\clearpage{}
\newgeometry{left=2.7cm, right=2.7cm, top=2cm}
\part{Appendix}
\label{part:cheat-sheet}

\section{Cheat sheet for stiffness matrix analysis}
\label{sec:cheat-sheet-stiffn}

\newenvironment{meine}{}{}
\newcommand{\mypmatrix}[1]
{
  \begin{meine}
    \renewcommand{\arraystretch}{1}
    \begin{pmatrix}
      #1
    \end{pmatrix}
    \renewcommand{\arraystretch}{2}
  \end{meine}
}

%\enlargethispage{2cm}
\begin{center}
  \begin{longtable}[ht!]{ccp{8cm}}
    \toprule
    Symbol & Definition & Notes \\
    \midrule{}

    $\phi_i(x,y)$ & & $i$th basis function, simpler notation: $\phi_i$ \\

    $\int_\Omega (\cdot)\, d\Omega$ & & simpler notation for 2-dimensional integration over area $\Omega$ \\

    $\mathbf{q}$ & $\mypmatrix{
      h \\ u_x \\ u_y
    }$ & Height, $x$ momentum and $y$ momentum grouped into one vector \\

    $e_x$ & $\mypmatrix{ 1 \\ 0 }$ & \\
    $e_y$ & $\mypmatrix{ 0 \\ 1 }$ & \\

    $\mathbf{u}$ & $ \mypmatrix{ u_x \\ u_y }$ & \\

    $\mathbf{q}_i$ & $\mypmatrix{
      h_i \\ u_{x,i} \\ u_{y,i}
    }$ & Same as before, but for $i$th support point  \\

    $F(\mathbf{q})$ &
    $\mypmatrix{ \mathbf{u} \\ \frac{u_x}{h}\mathbf{u} + \frac{1}{2} g h^2 e_x \\ \frac{u_y}{h}\mathbf{u} + \frac{1}{2} g h^2 e_y }$ &
    The three components can be extracted with $F_1(\mathbf{q})$, $F_2(\mathbf{q})$, $F_3(\mathbf{q})$ \\

    $\mathbf{q}$ & $\sum_{i=1}^n \mathbf{q}_i \phi_i$ & Approximation of $\mathbf{q}$, three components \\

    $\overline{F}_2(\mathbf{q})$ &
    $\mypmatrix{\sum_{j=1}^n \left( \frac{u_{x,j}^2}{h_j}+\frac{1}{2} g h_j^2  \right) \phi_j \\
    \sum_{j=1}^n \left( \frac{u_{x,j} \cdot u_{y,j}}{h_j} \right) \phi_j}$ &
    Point-wise approximation of second flux component \\

    $\overline{F}_3(\mathbf{q})$ &
    $\mypmatrix{
    \sum_{j=1}^n \left( \frac{u_{x,j} \cdot u_{y,j}}{h_j} \right) \phi_j \\
      \sum_{j=1}^n \left( \frac{u_{y,j}^2}{h_j}+\frac{1}{2} g h_j^2  \right) \phi_j
}$ &
    Point-wise approximation of third flux component \\

    $\widehat{h}$ & $\sum_{j=1}^n \phi_j h_j$ & Approximation of $h$ \\
    $\widehat{u}_x$ & $\sum_{j=1}^n \phi_j u_{x,j}$ & Approximation of $u_x$ \\
    $\widehat{u}_y$ & $\sum_{j=1}^n \phi_j u_{y,j}$ & Approximation of $u_y$ \\

    $\widehat{F}_2(\mathbf{q})$ & $\mypmatrix{
      \left( \frac{\widehat{u}_x^2}{\widehat{h}} + \frac{1}{2} g \widehat{h}^2 \right) \\
      \frac{\widehat{u}_x\cdot \widehat{u}_y}{\widehat{h}}
    }$ & Exact solution for second component of flux \\

    $\widehat{F}_3(\mathbf{q})$ & $\mypmatrix{
      \frac{\widehat{u}_x\cdot \widehat{u}_y}{\widehat{h}} \\
      \left( \frac{\widehat{u}_y^2}{\widehat{h}} + \frac{1}{2} g \widehat{h}^2 \right)
    }$ & Exact solution for second component of flux \\

    $\mypmatrix{A \\ B} \cdot \nabla\phi$ & $A \cdot \pd{\phi}{x} + B \cdot \pd{\phi}{y}$ & Needed, since each line of flux has two ``sub lines'' that have to be combined into one\\

    $SE_x^i$ & $\int_T \left| \left( \overline{F}_2(\mathbf{q}) - \widehat{F}_2(\mathbf{q}) \right) \cdot \nabla \phi_i \right| \,dT$ & Error term for $x$ momentum for $i$th basis function \\

    $SE_y^i$ & $\int_T \left| \left( \overline{F}_3(\mathbf{q}) - \widehat{F}_3(\mathbf{q}) \right) \cdot \nabla \phi_i \right| \,dT$ & Error term for $y$ momentum for $i$th basis function \\

    $SE$ & $\mypmatrix{SE_x \\ SE_y}$ & \\

    \bottomrule
  \end{longtable}
\end{center}

\end{document}


%%% Local Variables:
%%% TeX-master: "results.tex"
%%% End:

